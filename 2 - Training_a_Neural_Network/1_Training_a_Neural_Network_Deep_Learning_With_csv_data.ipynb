{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Neural Network"
      ],
      "metadata": {
        "id": "lHbwIOLWUtBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs:"
      ],
      "metadata": {
        "id": "aH9bPGn_U2aP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hib9nuB_UYSv"
      },
      "outputs": [],
      "source": [
        "# % pip install torch\n",
        "# % pip install sklearning\n",
        "# % pip install torchsummary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports:"
      ],
      "metadata": {
        "id": "k9Tue9WqVGon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch\n",
        "import torch\n",
        "from torch import nn # Neural Networks\n",
        "from torch.utils.data import Dataset # Splitting training data and ted data\n",
        "from torch.utils.data import DataLoader # Loading data train and data test\n",
        "import torch.nn.init # Weight initialization\n",
        "from torch import optim # Optimizer\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "1_Hdc0LrVEK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs:"
      ],
      "metadata": {
        "id": "0aPTBntdWe2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "Q14VS1bOWNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters and hyperparameters"
      ],
      "metadata": {
        "id": "zN8tZSSrW9mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'device': '',\n",
        "    'batch_size': 256,\n",
        "    'num_workers': 2,\n",
        "    'lr': 1e-5,\n",
        "    'weigth_decay': 5e-4,\n",
        "    'num_epochs': 5000\n",
        "}"
      ],
      "metadata": {
        "id": "pdsj7mMEWNKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the device"
      ],
      "metadata": {
        "id": "p3fTQOLpXLq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    args['device'] = 'cuda'\n",
        "else:\n",
        "    args['device'] = 'cpu'\n",
        "\n",
        "args['device']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CCxvalwHWNHT",
        "outputId": "14cac201-2a0c-4fb8-eb1a-f3b52150d001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "poyMl5fFXkzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading dataset\n",
        "#! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip"
      ],
      "metadata": {
        "id": "tbxyMQ_KWNEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-aeAodMWNB8",
        "outputId": "8d1df210-a065-4962-c584-f585b658adeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bike-Sharing-Dataset.zip  bike_validation.csv  hour.csv    sample_data\n",
            "bike_train.csv\t\t  day.csv\t       Readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! unzip Bike-Sharing-Dataset.zip"
      ],
      "metadata": {
        "id": "zZrH8gtdWM_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hour.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "gb88cbaPWM8U",
        "outputId": "3eb1fd25-65bc-4c4f-8b97-7476a304c456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
              "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
              "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
              "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
              "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
              "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
              "\n",
              "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
              "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
              "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
              "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
              "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
              "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d3f4431-2ae5-4361-a85a-e53e4e51f730\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d3f4431-2ae5-4361-a85a-e53e4e51f730')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d3f4431-2ae5-4361-a85a-e53e4e51f730 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d3f4431-2ae5-4361-a85a-e53e4e51f730');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-73226a39-128f-443a-a728-244a39e3df96\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-73226a39-128f-443a-a728-244a39e3df96')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-73226a39-128f-443a-a728-244a39e3df96 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 17379,\n  \"fields\": [\n    {\n      \"column\": \"instant\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5017,\n        \"min\": 1,\n        \"max\": 17379,\n        \"num_unique_values\": 17379,\n        \"samples\": [\n          12831,\n          8689,\n          7092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dteday\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 731,\n        \"samples\": [\n          \"2012-12-04\",\n          \"2011-02-03\",\n          \"2011-10-28\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"yr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mnth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 23,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          8,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"holiday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          6,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workingday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weathersit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1925561212497219,\n        \"min\": 0.02,\n        \"max\": 1.0,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.16,\n          0.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"atemp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17185021563535943,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 65,\n        \"samples\": [\n          0.7879,\n          0.9242\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19292983406291508,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          0.29,\n          0.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"windspeed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12234022857279049,\n        \"min\": 0.0,\n        \"max\": 0.8507,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          0.8507,\n          0.4925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"casual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49,\n        \"min\": 0,\n        \"max\": 367,\n        \"num_unique_values\": 322,\n        \"samples\": [\n          201,\n          171\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"registered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 151,\n        \"min\": 0,\n        \"max\": 886,\n        \"num_unique_values\": 776,\n        \"samples\": [\n          342,\n          744\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cnt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 181,\n        \"min\": 1,\n        \"max\": 977,\n        \"num_unique_values\": 869,\n        \"samples\": [\n          594,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data size\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ00d6TEWM5d",
        "outputId": "dc6c699f-d401-4768-b4a0-6f5e3dce82fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17379, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data info\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ3zJoO3WM0R",
        "outputId": "092907a1-7612-4215-8cfd-45c4b4840726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17379 entries, 0 to 17378\n",
            "Data columns (total 17 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   instant     17379 non-null  int64  \n",
            " 1   dteday      17379 non-null  object \n",
            " 2   season      17379 non-null  int64  \n",
            " 3   yr          17379 non-null  int64  \n",
            " 4   mnth        17379 non-null  int64  \n",
            " 5   hr          17379 non-null  int64  \n",
            " 6   holiday     17379 non-null  int64  \n",
            " 7   weekday     17379 non-null  int64  \n",
            " 8   workingday  17379 non-null  int64  \n",
            " 9   weathersit  17379 non-null  int64  \n",
            " 10  temp        17379 non-null  float64\n",
            " 11  atemp       17379 non-null  float64\n",
            " 12  hum         17379 non-null  float64\n",
            " 13  windspeed   17379 non-null  float64\n",
            " 14  casual      17379 non-null  int64  \n",
            " 15  registered  17379 non-null  int64  \n",
            " 16  cnt         17379 non-null  int64  \n",
            "dtypes: float64(4), int64(12), object(1)\n",
            "memory usage: 2.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data describe\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "emfBOGzPWMs5",
        "outputId": "7273f15e-8e9e-4a85-af55-8006c6b506d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          instant        season            yr          mnth            hr  \\\n",
              "count  17379.0000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean    8690.0000      2.501640      0.502561      6.537775     11.546752   \n",
              "std     5017.0295      1.106918      0.500008      3.438776      6.914405   \n",
              "min        1.0000      1.000000      0.000000      1.000000      0.000000   \n",
              "25%     4345.5000      2.000000      0.000000      4.000000      6.000000   \n",
              "50%     8690.0000      3.000000      1.000000      7.000000     12.000000   \n",
              "75%    13034.5000      3.000000      1.000000     10.000000     18.000000   \n",
              "max    17379.0000      4.000000      1.000000     12.000000     23.000000   \n",
              "\n",
              "            holiday       weekday    workingday    weathersit          temp  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.028770      3.003683      0.682721      1.425283      0.496987   \n",
              "std        0.167165      2.005771      0.465431      0.639357      0.192556   \n",
              "min        0.000000      0.000000      0.000000      1.000000      0.020000   \n",
              "25%        0.000000      1.000000      0.000000      1.000000      0.340000   \n",
              "50%        0.000000      3.000000      1.000000      1.000000      0.500000   \n",
              "75%        0.000000      5.000000      1.000000      2.000000      0.660000   \n",
              "max        1.000000      6.000000      1.000000      4.000000      1.000000   \n",
              "\n",
              "              atemp           hum     windspeed        casual    registered  \\\n",
              "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
              "mean       0.475775      0.627229      0.190098     35.676218    153.786869   \n",
              "std        0.171850      0.192930      0.122340     49.305030    151.357286   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.333300      0.480000      0.104500      4.000000     34.000000   \n",
              "50%        0.484800      0.630000      0.194000     17.000000    115.000000   \n",
              "75%        0.621200      0.780000      0.253700     48.000000    220.000000   \n",
              "max        1.000000      1.000000      0.850700    367.000000    886.000000   \n",
              "\n",
              "                cnt  \n",
              "count  17379.000000  \n",
              "mean     189.463088  \n",
              "std      181.387599  \n",
              "min        1.000000  \n",
              "25%       40.000000  \n",
              "50%      142.000000  \n",
              "75%      281.000000  \n",
              "max      977.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-249d2454-8d25-4ab9-9c8d-eb2fd649edb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17379.0000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "      <td>17379.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8690.0000</td>\n",
              "      <td>2.501640</td>\n",
              "      <td>0.502561</td>\n",
              "      <td>6.537775</td>\n",
              "      <td>11.546752</td>\n",
              "      <td>0.028770</td>\n",
              "      <td>3.003683</td>\n",
              "      <td>0.682721</td>\n",
              "      <td>1.425283</td>\n",
              "      <td>0.496987</td>\n",
              "      <td>0.475775</td>\n",
              "      <td>0.627229</td>\n",
              "      <td>0.190098</td>\n",
              "      <td>35.676218</td>\n",
              "      <td>153.786869</td>\n",
              "      <td>189.463088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5017.0295</td>\n",
              "      <td>1.106918</td>\n",
              "      <td>0.500008</td>\n",
              "      <td>3.438776</td>\n",
              "      <td>6.914405</td>\n",
              "      <td>0.167165</td>\n",
              "      <td>2.005771</td>\n",
              "      <td>0.465431</td>\n",
              "      <td>0.639357</td>\n",
              "      <td>0.192556</td>\n",
              "      <td>0.171850</td>\n",
              "      <td>0.192930</td>\n",
              "      <td>0.122340</td>\n",
              "      <td>49.305030</td>\n",
              "      <td>151.357286</td>\n",
              "      <td>181.387599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4345.5000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.333300</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8690.0000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.484800</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>142.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>13034.5000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.621200</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.253700</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>220.000000</td>\n",
              "      <td>281.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17379.0000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850700</td>\n",
              "      <td>367.000000</td>\n",
              "      <td>886.000000</td>\n",
              "      <td>977.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-249d2454-8d25-4ab9-9c8d-eb2fd649edb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-249d2454-8d25-4ab9-9c8d-eb2fd649edb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-249d2454-8d25-4ab9-9c8d-eb2fd649edb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-709cb135-660c-4e8c-b931-37bc3b09c489\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-709cb135-660c-4e8c-b931-37bc3b09c489')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-709cb135-660c-4e8c-b931-37bc3b09c489 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"instant\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6263.170885696781,\n        \"min\": 1.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          17379.0,\n          8690.0,\n          13034.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6143.565598544762,\n        \"min\": 1.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          17379.0,\n          2.5016399102364923,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"yr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.202229000585,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5025605615973301,\n          1.0,\n          0.5000078290910197\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mnth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6142.184250617928,\n        \"min\": 1.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.537775476149376,\n          7.0,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6140.496148184537,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          11.546751826917545,\n          12.0,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"holiday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.34398083374,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.028770355026181024,\n          1.0,\n          0.16716527638437123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6143.394057236404,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.003682605443351,\n          3.0,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workingday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.194876084175,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6827205247712756,\n          1.0,\n          0.46543063352388286\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weathersit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6143.845618830189,\n        \"min\": 0.6393568777542534,\n        \"max\": 17379.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          17379.0,\n          1.425283387997008,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.242275843299,\n        \"min\": 0.02,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.4969871684216583,\n          0.5,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"atemp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.248469131704,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.4757751021347604,\n          0.4848,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.216991945488,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.6272288394038783,\n          0.63,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"windspeed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6144.317742909861,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.1900976063064618,\n          0.194,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"casual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6119.284233238239,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          35.67621842453536,\n          17.0,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"registered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6072.038722856437,\n        \"min\": 0.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          153.78686920996606,\n          115.0,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cnt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6060.617601280442,\n        \"min\": 1.0,\n        \"max\": 17379.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          189.46308763450142,\n          142.0,\n          17379.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the data"
      ],
      "metadata": {
        "id": "F2FEccz8Zg24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing columns that are not relevant to our training model\n",
        "df = df.drop(columns= ['instant', 'dteday', 'casual', 'registered'])"
      ],
      "metadata": {
        "id": "EYlz-iTlXi6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot"
      ],
      "metadata": {
        "id": "woaRCwocaNAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining categorical variations\n",
        "categorical_columns = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
        "\n",
        "# Empty list to store the resulting DataFrames after applying one-hot encoding.\n",
        "one_hot_dfs = []\n",
        "\n",
        "# Apply one-hot encoding with get_dummies\n",
        "for col in categorical_columns:\n",
        "    one_hot_df = pd.get_dummies(df[col], prefix = col).astype(int)\n",
        "    one_hot_dfs.append(one_hot_df)\n",
        "\n",
        "# Concatenate the one-hot representations with the other columns of the DataFrame\n",
        "one_hot_df = pd.concat([df.drop(columns=categorical_columns)] + one_hot_dfs, axis = 1)"
      ],
      "metadata": {
        "id": "Q2W5n3JvdOgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "JnqX8GXeXi0q",
        "outputId": "8e4f96f5-130c-4a78-d03d-5d95c11786bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   temp   atemp   hum  windspeed  cnt  season_1  season_2  season_3  season_4  \\\n",
              "0  0.24  0.2879  0.81        0.0   16         1         0         0         0   \n",
              "1  0.22  0.2727  0.80        0.0   40         1         0         0         0   \n",
              "2  0.22  0.2727  0.80        0.0   32         1         0         0         0   \n",
              "3  0.24  0.2879  0.75        0.0   13         1         0         0         0   \n",
              "4  0.24  0.2879  0.75        0.0    1         1         0         0         0   \n",
              "\n",
              "   yr_0  yr_1  mnth_1  mnth_2  mnth_3  mnth_4  mnth_5  mnth_6  mnth_7  mnth_8  \\\n",
              "0     1     0       1       0       0       0       0       0       0       0   \n",
              "1     1     0       1       0       0       0       0       0       0       0   \n",
              "2     1     0       1       0       0       0       0       0       0       0   \n",
              "3     1     0       1       0       0       0       0       0       0       0   \n",
              "4     1     0       1       0       0       0       0       0       0       0   \n",
              "\n",
              "   mnth_9  mnth_10  mnth_11  mnth_12  hr_0  hr_1  hr_2  hr_3  hr_4  hr_5  \\\n",
              "0       0        0        0        0     1     0     0     0     0     0   \n",
              "1       0        0        0        0     0     1     0     0     0     0   \n",
              "2       0        0        0        0     0     0     1     0     0     0   \n",
              "3       0        0        0        0     0     0     0     1     0     0   \n",
              "4       0        0        0        0     0     0     0     0     1     0   \n",
              "\n",
              "   hr_6  hr_7  hr_8  hr_9  hr_10  hr_11  hr_12  hr_13  hr_14  hr_15  hr_16  \\\n",
              "0     0     0     0     0      0      0      0      0      0      0      0   \n",
              "1     0     0     0     0      0      0      0      0      0      0      0   \n",
              "2     0     0     0     0      0      0      0      0      0      0      0   \n",
              "3     0     0     0     0      0      0      0      0      0      0      0   \n",
              "4     0     0     0     0      0      0      0      0      0      0      0   \n",
              "\n",
              "   hr_17  hr_18  hr_19  hr_20  hr_21  hr_22  hr_23  holiday_0  holiday_1  \\\n",
              "0      0      0      0      0      0      0      0          1          0   \n",
              "1      0      0      0      0      0      0      0          1          0   \n",
              "2      0      0      0      0      0      0      0          1          0   \n",
              "3      0      0      0      0      0      0      0          1          0   \n",
              "4      0      0      0      0      0      0      0          1          0   \n",
              "\n",
              "   weekday_0  weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  \\\n",
              "0          0          0          0          0          0          0   \n",
              "1          0          0          0          0          0          0   \n",
              "2          0          0          0          0          0          0   \n",
              "3          0          0          0          0          0          0   \n",
              "4          0          0          0          0          0          0   \n",
              "\n",
              "   weekday_6  workingday_0  workingday_1  weathersit_1  weathersit_2  \\\n",
              "0          1             1             0             1             0   \n",
              "1          1             1             0             1             0   \n",
              "2          1             1             0             1             0   \n",
              "3          1             1             0             1             0   \n",
              "4          1             1             0             1             0   \n",
              "\n",
              "   weathersit_3  weathersit_4  \n",
              "0             0             0  \n",
              "1             0             0  \n",
              "2             0             0  \n",
              "3             0             0  \n",
              "4             0             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-923ecba2-91ed-4a12-943d-6b7b14e63660\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>cnt</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>yr_0</th>\n",
              "      <th>yr_1</th>\n",
              "      <th>mnth_1</th>\n",
              "      <th>mnth_2</th>\n",
              "      <th>mnth_3</th>\n",
              "      <th>mnth_4</th>\n",
              "      <th>mnth_5</th>\n",
              "      <th>mnth_6</th>\n",
              "      <th>mnth_7</th>\n",
              "      <th>mnth_8</th>\n",
              "      <th>mnth_9</th>\n",
              "      <th>mnth_10</th>\n",
              "      <th>mnth_11</th>\n",
              "      <th>mnth_12</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>hr_2</th>\n",
              "      <th>hr_3</th>\n",
              "      <th>hr_4</th>\n",
              "      <th>hr_5</th>\n",
              "      <th>hr_6</th>\n",
              "      <th>hr_7</th>\n",
              "      <th>hr_8</th>\n",
              "      <th>hr_9</th>\n",
              "      <th>hr_10</th>\n",
              "      <th>hr_11</th>\n",
              "      <th>hr_12</th>\n",
              "      <th>hr_13</th>\n",
              "      <th>hr_14</th>\n",
              "      <th>hr_15</th>\n",
              "      <th>hr_16</th>\n",
              "      <th>hr_17</th>\n",
              "      <th>hr_18</th>\n",
              "      <th>hr_19</th>\n",
              "      <th>hr_20</th>\n",
              "      <th>hr_21</th>\n",
              "      <th>hr_22</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>holiday_0</th>\n",
              "      <th>holiday_1</th>\n",
              "      <th>weekday_0</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>workingday_0</th>\n",
              "      <th>workingday_1</th>\n",
              "      <th>weathersit_1</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>weathersit_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-923ecba2-91ed-4a12-943d-6b7b14e63660')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-923ecba2-91ed-4a12-943d-6b7b14e63660 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-923ecba2-91ed-4a12-943d-6b7b14e63660');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e0bc755b-434e-43e8-8640-d34657a970c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0bc755b-434e-43e8-8640-d34657a970c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e0bc755b-434e-43e8-8640-d34657a970c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "one_hot_df"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bODHbnT4Y2mx",
        "outputId": "650e503c-ea9a-401d-800e-28b8a7e7ee8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17379, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_df.insert(62, 'label', one_hot_df['cnt'])\n",
        "one_hot_df = one_hot_df.drop(columns=['cnt'])\n",
        "one_hot_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "uGLzNBPzY2kk",
        "outputId": "2b1cdf2f-4298-470b-fa90-f45068c4095d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   temp   atemp   hum  windspeed  season_1  season_2  season_3  season_4  \\\n",
              "0  0.24  0.2879  0.81        0.0         1         0         0         0   \n",
              "1  0.22  0.2727  0.80        0.0         1         0         0         0   \n",
              "2  0.22  0.2727  0.80        0.0         1         0         0         0   \n",
              "3  0.24  0.2879  0.75        0.0         1         0         0         0   \n",
              "4  0.24  0.2879  0.75        0.0         1         0         0         0   \n",
              "\n",
              "   yr_0  yr_1  mnth_1  mnth_2  mnth_3  mnth_4  mnth_5  mnth_6  mnth_7  mnth_8  \\\n",
              "0     1     0       1       0       0       0       0       0       0       0   \n",
              "1     1     0       1       0       0       0       0       0       0       0   \n",
              "2     1     0       1       0       0       0       0       0       0       0   \n",
              "3     1     0       1       0       0       0       0       0       0       0   \n",
              "4     1     0       1       0       0       0       0       0       0       0   \n",
              "\n",
              "   mnth_9  mnth_10  mnth_11  mnth_12  hr_0  hr_1  hr_2  hr_3  hr_4  hr_5  \\\n",
              "0       0        0        0        0     1     0     0     0     0     0   \n",
              "1       0        0        0        0     0     1     0     0     0     0   \n",
              "2       0        0        0        0     0     0     1     0     0     0   \n",
              "3       0        0        0        0     0     0     0     1     0     0   \n",
              "4       0        0        0        0     0     0     0     0     1     0   \n",
              "\n",
              "   hr_6  hr_7  hr_8  hr_9  hr_10  hr_11  hr_12  hr_13  hr_14  hr_15  hr_16  \\\n",
              "0     0     0     0     0      0      0      0      0      0      0      0   \n",
              "1     0     0     0     0      0      0      0      0      0      0      0   \n",
              "2     0     0     0     0      0      0      0      0      0      0      0   \n",
              "3     0     0     0     0      0      0      0      0      0      0      0   \n",
              "4     0     0     0     0      0      0      0      0      0      0      0   \n",
              "\n",
              "   hr_17  hr_18  hr_19  hr_20  hr_21  hr_22  hr_23  holiday_0  holiday_1  \\\n",
              "0      0      0      0      0      0      0      0          1          0   \n",
              "1      0      0      0      0      0      0      0          1          0   \n",
              "2      0      0      0      0      0      0      0          1          0   \n",
              "3      0      0      0      0      0      0      0          1          0   \n",
              "4      0      0      0      0      0      0      0          1          0   \n",
              "\n",
              "   weekday_0  weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  \\\n",
              "0          0          0          0          0          0          0   \n",
              "1          0          0          0          0          0          0   \n",
              "2          0          0          0          0          0          0   \n",
              "3          0          0          0          0          0          0   \n",
              "4          0          0          0          0          0          0   \n",
              "\n",
              "   weekday_6  workingday_0  workingday_1  weathersit_1  weathersit_2  \\\n",
              "0          1             1             0             1             0   \n",
              "1          1             1             0             1             0   \n",
              "2          1             1             0             1             0   \n",
              "3          1             1             0             1             0   \n",
              "4          1             1             0             1             0   \n",
              "\n",
              "   weathersit_3  weathersit_4  label  \n",
              "0             0             0     16  \n",
              "1             0             0     40  \n",
              "2             0             0     32  \n",
              "3             0             0     13  \n",
              "4             0             0      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d10cd27e-e05c-4ae8-9ad9-2b639e721ba0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>yr_0</th>\n",
              "      <th>yr_1</th>\n",
              "      <th>mnth_1</th>\n",
              "      <th>mnth_2</th>\n",
              "      <th>mnth_3</th>\n",
              "      <th>mnth_4</th>\n",
              "      <th>mnth_5</th>\n",
              "      <th>mnth_6</th>\n",
              "      <th>mnth_7</th>\n",
              "      <th>mnth_8</th>\n",
              "      <th>mnth_9</th>\n",
              "      <th>mnth_10</th>\n",
              "      <th>mnth_11</th>\n",
              "      <th>mnth_12</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>hr_2</th>\n",
              "      <th>hr_3</th>\n",
              "      <th>hr_4</th>\n",
              "      <th>hr_5</th>\n",
              "      <th>hr_6</th>\n",
              "      <th>hr_7</th>\n",
              "      <th>hr_8</th>\n",
              "      <th>hr_9</th>\n",
              "      <th>hr_10</th>\n",
              "      <th>hr_11</th>\n",
              "      <th>hr_12</th>\n",
              "      <th>hr_13</th>\n",
              "      <th>hr_14</th>\n",
              "      <th>hr_15</th>\n",
              "      <th>hr_16</th>\n",
              "      <th>hr_17</th>\n",
              "      <th>hr_18</th>\n",
              "      <th>hr_19</th>\n",
              "      <th>hr_20</th>\n",
              "      <th>hr_21</th>\n",
              "      <th>hr_22</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>holiday_0</th>\n",
              "      <th>holiday_1</th>\n",
              "      <th>weekday_0</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>workingday_0</th>\n",
              "      <th>workingday_1</th>\n",
              "      <th>weathersit_1</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>weathersit_4</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d10cd27e-e05c-4ae8-9ad9-2b639e721ba0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d10cd27e-e05c-4ae8-9ad9-2b639e721ba0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d10cd27e-e05c-4ae8-9ad9-2b639e721ba0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3ddef854-e10e-4a8b-871c-a17f317ccdd0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3ddef854-e10e-4a8b-871c-a17f317ccdd0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3ddef854-e10e-4a8b-871c-a17f317ccdd0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "one_hot_df"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vi2sAkhY2iF",
        "outputId": "6387b1bc-7829-4a86-b44c-e602084cc686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17379, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = one_hot_df"
      ],
      "metadata": {
        "id": "76zivyj4jwlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing numerical variables"
      ],
      "metadata": {
        "id": "1LnYtTeyiqBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric columns to normalize\n",
        "normalize_cols = ['temp', 'atemp', 'hum', 'windspeed']\n",
        "\n",
        "# Storing the normalization data\n",
        "col_scaler_store = {}\n",
        "\n",
        "# Normalizing data\n",
        "for col in normalize_cols:\n",
        "    mean, std = data[col].mean(), data[col].std()\n",
        "    col_scaler_store[col] = [mean, std]\n",
        "    data.loc[:, col] = (data[col] - mean) / std\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Hftyr_aOY2f9",
        "outputId": "24ac6d9f-31d9-4959-a663-1f8fb73ffd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       temp     atemp       hum  windspeed  season_1  season_2  season_3  \\\n",
              "0 -1.334609 -1.093249  0.947345  -1.553844         1         0         0   \n",
              "1 -1.438475 -1.181698  0.895513  -1.553844         1         0         0   \n",
              "2 -1.438475 -1.181698  0.895513  -1.553844         1         0         0   \n",
              "3 -1.334609 -1.093249  0.636351  -1.553844         1         0         0   \n",
              "4 -1.334609 -1.093249  0.636351  -1.553844         1         0         0   \n",
              "\n",
              "   season_4  yr_0  yr_1  mnth_1  mnth_2  mnth_3  mnth_4  mnth_5  mnth_6  \\\n",
              "0         0     1     0       1       0       0       0       0       0   \n",
              "1         0     1     0       1       0       0       0       0       0   \n",
              "2         0     1     0       1       0       0       0       0       0   \n",
              "3         0     1     0       1       0       0       0       0       0   \n",
              "4         0     1     0       1       0       0       0       0       0   \n",
              "\n",
              "   mnth_7  mnth_8  mnth_9  mnth_10  mnth_11  mnth_12  hr_0  hr_1  hr_2  hr_3  \\\n",
              "0       0       0       0        0        0        0     1     0     0     0   \n",
              "1       0       0       0        0        0        0     0     1     0     0   \n",
              "2       0       0       0        0        0        0     0     0     1     0   \n",
              "3       0       0       0        0        0        0     0     0     0     1   \n",
              "4       0       0       0        0        0        0     0     0     0     0   \n",
              "\n",
              "   hr_4  hr_5  hr_6  hr_7  hr_8  hr_9  hr_10  hr_11  hr_12  hr_13  hr_14  \\\n",
              "0     0     0     0     0     0     0      0      0      0      0      0   \n",
              "1     0     0     0     0     0     0      0      0      0      0      0   \n",
              "2     0     0     0     0     0     0      0      0      0      0      0   \n",
              "3     0     0     0     0     0     0      0      0      0      0      0   \n",
              "4     1     0     0     0     0     0      0      0      0      0      0   \n",
              "\n",
              "   hr_15  hr_16  hr_17  hr_18  hr_19  hr_20  hr_21  hr_22  hr_23  holiday_0  \\\n",
              "0      0      0      0      0      0      0      0      0      0          1   \n",
              "1      0      0      0      0      0      0      0      0      0          1   \n",
              "2      0      0      0      0      0      0      0      0      0          1   \n",
              "3      0      0      0      0      0      0      0      0      0          1   \n",
              "4      0      0      0      0      0      0      0      0      0          1   \n",
              "\n",
              "   holiday_1  weekday_0  weekday_1  weekday_2  weekday_3  weekday_4  \\\n",
              "0          0          0          0          0          0          0   \n",
              "1          0          0          0          0          0          0   \n",
              "2          0          0          0          0          0          0   \n",
              "3          0          0          0          0          0          0   \n",
              "4          0          0          0          0          0          0   \n",
              "\n",
              "   weekday_5  weekday_6  workingday_0  workingday_1  weathersit_1  \\\n",
              "0          0          1             1             0             1   \n",
              "1          0          1             1             0             1   \n",
              "2          0          1             1             0             1   \n",
              "3          0          1             1             0             1   \n",
              "4          0          1             1             0             1   \n",
              "\n",
              "   weathersit_2  weathersit_3  weathersit_4  label  \n",
              "0             0             0             0     16  \n",
              "1             0             0             0     40  \n",
              "2             0             0             0     32  \n",
              "3             0             0             0     13  \n",
              "4             0             0             0      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-897407b0-3400-4681-94fa-41fa44f76297\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>yr_0</th>\n",
              "      <th>yr_1</th>\n",
              "      <th>mnth_1</th>\n",
              "      <th>mnth_2</th>\n",
              "      <th>mnth_3</th>\n",
              "      <th>mnth_4</th>\n",
              "      <th>mnth_5</th>\n",
              "      <th>mnth_6</th>\n",
              "      <th>mnth_7</th>\n",
              "      <th>mnth_8</th>\n",
              "      <th>mnth_9</th>\n",
              "      <th>mnth_10</th>\n",
              "      <th>mnth_11</th>\n",
              "      <th>mnth_12</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>hr_2</th>\n",
              "      <th>hr_3</th>\n",
              "      <th>hr_4</th>\n",
              "      <th>hr_5</th>\n",
              "      <th>hr_6</th>\n",
              "      <th>hr_7</th>\n",
              "      <th>hr_8</th>\n",
              "      <th>hr_9</th>\n",
              "      <th>hr_10</th>\n",
              "      <th>hr_11</th>\n",
              "      <th>hr_12</th>\n",
              "      <th>hr_13</th>\n",
              "      <th>hr_14</th>\n",
              "      <th>hr_15</th>\n",
              "      <th>hr_16</th>\n",
              "      <th>hr_17</th>\n",
              "      <th>hr_18</th>\n",
              "      <th>hr_19</th>\n",
              "      <th>hr_20</th>\n",
              "      <th>hr_21</th>\n",
              "      <th>hr_22</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>holiday_0</th>\n",
              "      <th>holiday_1</th>\n",
              "      <th>weekday_0</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>workingday_0</th>\n",
              "      <th>workingday_1</th>\n",
              "      <th>weathersit_1</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>weathersit_4</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.334609</td>\n",
              "      <td>-1.093249</td>\n",
              "      <td>0.947345</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.438475</td>\n",
              "      <td>-1.181698</td>\n",
              "      <td>0.895513</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.438475</td>\n",
              "      <td>-1.181698</td>\n",
              "      <td>0.895513</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.334609</td>\n",
              "      <td>-1.093249</td>\n",
              "      <td>0.636351</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.334609</td>\n",
              "      <td>-1.093249</td>\n",
              "      <td>0.636351</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-897407b0-3400-4681-94fa-41fa44f76297')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-897407b0-3400-4681-94fa-41fa44f76297 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-897407b0-3400-4681-94fa-41fa44f76297');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e65d8356-c5ed-465f-91d6-95ab19b89846\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e65d8356-c5ed-465f-91d6-95ab19b89846')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e65d8356-c5ed-465f-91d6-95ab19b89846 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_scaler_store\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS2Ru5IIjAEJ",
        "outputId": "e20c3a84-1a0c-465f-d488-18a76043b9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'temp': [0.4969871684216583, 0.1925561212497219],\n",
              " 'atemp': [0.4757751021347604, 0.17185021563535943],\n",
              " 'hum': [0.6272288394038783, 0.19292983406291508],\n",
              " 'windspeed': [0.1900976063064618, 0.12234022857279049]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separating training data and test data"
      ],
      "metadata": {
        "id": "y88QfmeNlM94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining seed\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Index Pytorch\n",
        "index = torch.randperm(len(data)).tolist()\n",
        "\n",
        "# Train size\n",
        "train_size = int(0.8 * len(data))\n",
        "\n",
        "df_train = data.iloc[index[: train_size]]\n",
        "df_validation = data.iloc[index[train_size:]]"
      ],
      "metadata": {
        "id": "nqAoJ_KqY2de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train size\n",
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azGa-1wwY2a8",
        "outputId": "b389d354-c4ab-4831-e513-798c21257bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13903, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test size\n",
        "df_validation.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njhIhoJkY2Yb",
        "outputId": "ef90366d-9d4f-478a-ca77-f8eceb4c3497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3476, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Dataframe training and test"
      ],
      "metadata": {
        "id": "c-pOuBUnnb3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv('bike_train.csv', index = False)\n",
        "df_validation.to_csv('bike_validation.csv', index = False)\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFGtYCJ2Y2V-",
        "outputId": "74d70d74-122a-4bd6-d394-226747380859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bike-Sharing-Dataset.zip  bike_validation.csv  hour.csv    sample_data\n",
            "bike_train.csv\t\t  day.csv\t       Readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Dataset"
      ],
      "metadata": {
        "id": "j-HbBhvsn4EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetBike(Dataset):\n",
        "    def __init__(self, csv_path, scaler_feat = None, scaler_label = None):\n",
        "\n",
        "        # Read csv file\n",
        "        self.data = pd.read_csv(csv_path).to_numpy()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Separating samples\n",
        "        sample = self.data[idx][:61]\n",
        "        label = self.data[idx][-1:]\n",
        "\n",
        "        # Transforming in tensors\n",
        "        sample = torch.from_numpy(sample.astype(np.float32))\n",
        "        label = torch.from_numpy(label.astype(np.int64))\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "UR_nUQ1RY2Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = DatasetBike('bike_train.csv')\n",
        "validation_set = DatasetBike('bike_validation.csv')"
      ],
      "metadata": {
        "id": "2-hAwIaXY2Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = train_set[0]\n",
        "data, label\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhOqHjL3Y2OS",
        "outputId": "482b3078-1613-4952-e70f-babcfae71744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.6075, -0.4764, -1.8516,  1.3741,  0.0000,  0.0000,  0.0000,  1.0000,\n",
              "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
              "          1.0000,  1.0000,  0.0000,  0.0000,  0.0000]),\n",
              " tensor([373]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "xPhmhH4ntMN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining train loader\n",
        "train_loader = DataLoader(\n",
        "    dataset = train_set,\n",
        "    batch_size = args['batch_size'],\n",
        "    shuffle = True,\n",
        "    num_workers = args['num_workers']\n",
        ")"
      ],
      "metadata": {
        "id": "FZ9IhbvPY2JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining test loader\n",
        "\n",
        "validation_loader = DataLoader(\n",
        "    dataset = validation_set,\n",
        "    batch_size = args['batch_size'],\n",
        "    shuffle = True,\n",
        "    num_workers = args['num_workers']\n",
        ")"
      ],
      "metadata": {
        "id": "JqgLLJ3tY2Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing train_loader data\n",
        "for batch in train_loader:\n",
        "    data, label = batch\n",
        "    print(data.size(), label.size())\n",
        "    break"
      ],
      "metadata": {
        "id": "hqQ5NQNpY2ET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f090c7ed-68b3-4780-fcaa-4edec19d6df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 61]) torch.Size([256, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating neural network"
      ],
      "metadata": {
        "id": "go_wAxv8e4NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assembling network model\n",
        "class NetWork(nn.Module):\n",
        "\n",
        "    # Initialization function\n",
        "    def __init__(self, input_size, out_size, dropout_prob = 0.5):\n",
        "\n",
        "        # __init__()\n",
        "        super(NetWork, self).__init__()\n",
        "\n",
        "        # Defining neural network layers\n",
        "        self.layer_set = nn.Sequential(\n",
        "\n",
        "            # Input Layer + Hidden Layer_0\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(dropout_prob),\n",
        "\n",
        "            # Hidden Layer_1\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(dropout_prob)\n",
        "\n",
        "            # Hidden Layer_2\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "            #nn.Dropout(dropout_prob)\n",
        "        )\n",
        "        # He initialization for the linear layers\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode = 'fan_in', nonlinearity = 'relu')\n",
        "\n",
        "        # Out Layer\n",
        "        self.out = nn.Linear(32, out_size)\n",
        "\n",
        "        # Forward\n",
        "    def forward(self, X):\n",
        "\n",
        "        # Initialization\n",
        "        layer_set = self.layer_set(X)\n",
        "\n",
        "        # Output data\n",
        "        output = self.out(layer_set)\n",
        "\n",
        "        # Returning the output data\n",
        "        return output"
      ],
      "metadata": {
        "id": "1amSjeD5Y2Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Network"
      ],
      "metadata": {
        "id": "bhYks7Shj5il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = NetWork(\n",
        "    input_size = len(train_set[0][0]), # Input Layer size\n",
        "    out_size = 1 # Variables that will be predicted\n",
        ").to(args['device'])"
      ],
      "metadata": {
        "id": "3VMzV25pY1_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net"
      ],
      "metadata": {
        "id": "zebf0vMnY18a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69678639-3cec-401a-df44-590f06293cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetWork(\n",
              "  (layer_set): Sequential(\n",
              "    (0): Linear(in_features=61, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "ohgx67a9kymA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr = args['lr'], weight_decay = args['weigth_decay'])"
      ],
      "metadata": {
        "id": "MaJ2CtlbY150"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "O657qPZHmr_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.L1Loss().to(args['device'])"
      ],
      "metadata": {
        "id": "90Wu2KdSY13F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Flow"
      ],
      "metadata": {
        "id": "6pzdkS1Zm5Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function training\n",
        "def training(train_loader, net, epoch):\n",
        "\n",
        "     # Setting training mode to network\n",
        "     net.train()\n",
        "\n",
        "     # Errors are stored in a list\n",
        "     epochs_loss = []\n",
        "\n",
        "     # Batchs\n",
        "     for batch in train_loader:\n",
        "\n",
        "        # Separating data and labels\n",
        "        data, label = batch\n",
        "\n",
        "        # Cast in Device(GPU/CPU)\n",
        "        data = data.to(args['device'])\n",
        "        label = label.to(args['device'])\n",
        "\n",
        "        # Forward\n",
        "        pred = net(data) # Prediction\n",
        "        loss = criterion(pred, label) # Calculate errors\n",
        "        epochs_loss.append(loss.cpu().data) # Saving training error data\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "     # Epochs_loss in to Numpy\n",
        "     epochs_loss = np.asarray(epochs_loss)\n",
        "\n",
        "     # Viewing training progress\n",
        "     print(10*'##', '-   Training   -', 10*'##')\n",
        "     print(f'Epoch: {epoch} - Loss Mean: {epochs_loss.mean():.4f} ----- [+ -]         {epochs_loss.std():.4f}')\n",
        "\n",
        "     # Return mean loss epochs\n",
        "     return epochs_loss.mean()"
      ],
      "metadata": {
        "id": "RH5Dv2lJY10L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function validation\n",
        "def validation(validation_loader, net, epoch):\n",
        "\n",
        "    # Setting test mode to network\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Errors are stored in a list\n",
        "        epochs_loss = []\n",
        "\n",
        "        # Batchs\n",
        "        for batch in validation_loader:\n",
        "\n",
        "            # Separating data and labels\n",
        "            data, label = batch\n",
        "\n",
        "            # Cast in Device(GPU/CPU)\n",
        "            data = data.to(args['device'])\n",
        "            label = label.to(args['device'])\n",
        "\n",
        "            # Forward\n",
        "            pred = net(data) # Prediction\n",
        "            loss = criterion(pred, label) # Calculate erros\n",
        "            epochs_loss.append(loss.cpu().data) # Saving training error data\n",
        "\n",
        "    # Epochs_loss in to Numpy\n",
        "    epochs_loss = np.asarray(epochs_loss)\n",
        "\n",
        "    # Viewing training progress\n",
        "    print(10*'##', '-  Validation  -', 10*'##')\n",
        "    print(f'Epoch: {epoch} -- Loss: {epochs_loss.mean():.4f} ----- [+ -]             {epochs_loss.std():.4f}')\n",
        "\n",
        "    # Return mean loss epochs\n",
        "    return epochs_loss.mean()"
      ],
      "metadata": {
        "id": "fNiC9_R3Y1xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "S5YQWip6s6xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss, validation_loss = [], []\n",
        "# Training + test(validation)\n",
        "for epoch in range(args['num_epochs']):\n",
        "\n",
        "    # Training\n",
        "    training_loss.append(training(train_loader = train_loader, net = net, epoch = epoch))\n",
        "\n",
        "    # Validation\n",
        "    validation_loss.append(validation(validation_loader = validation_loader, net = net, epoch = epoch))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "xx5IYjUkY1u8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88011694-f5f9-401e-d687-2c081103eb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4167 - Loss Mean: 23.4253 ----- [+ -]         2.1050\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4167 -- Loss: 27.1412 ----- [+ -]             2.4035\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4168 - Loss Mean: 23.0018 ----- [+ -]         2.0487\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4168 -- Loss: 26.5618 ----- [+ -]             1.9509\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4169 - Loss Mean: 22.5412 ----- [+ -]         2.1135\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4169 -- Loss: 26.2890 ----- [+ -]             2.2254\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4170 - Loss Mean: 22.3291 ----- [+ -]         1.8786\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4170 -- Loss: 26.0418 ----- [+ -]             2.3005\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4171 - Loss Mean: 22.4564 ----- [+ -]         2.0541\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4171 -- Loss: 26.1544 ----- [+ -]             2.0522\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4172 - Loss Mean: 22.8799 ----- [+ -]         1.6738\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4172 -- Loss: 26.3032 ----- [+ -]             2.2904\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4173 - Loss Mean: 23.2722 ----- [+ -]         1.7763\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4173 -- Loss: 26.5220 ----- [+ -]             2.1207\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4174 - Loss Mean: 23.5775 ----- [+ -]         1.7059\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4174 -- Loss: 26.6715 ----- [+ -]             2.2060\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4175 - Loss Mean: 23.5809 ----- [+ -]         1.6513\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4175 -- Loss: 26.4747 ----- [+ -]             2.2909\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4176 - Loss Mean: 23.4335 ----- [+ -]         1.7740\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4176 -- Loss: 26.2390 ----- [+ -]             2.6433\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4177 - Loss Mean: 22.9943 ----- [+ -]         2.1032\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4177 -- Loss: 26.1656 ----- [+ -]             1.9305\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4178 - Loss Mean: 22.5089 ----- [+ -]         1.4760\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4178 -- Loss: 25.8510 ----- [+ -]             2.0588\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4179 - Loss Mean: 22.3975 ----- [+ -]         2.0451\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4179 -- Loss: 26.2347 ----- [+ -]             2.0676\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4180 - Loss Mean: 22.4959 ----- [+ -]         1.6951\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4180 -- Loss: 26.4122 ----- [+ -]             2.0218\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4181 - Loss Mean: 22.9713 ----- [+ -]         2.0415\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4181 -- Loss: 26.9186 ----- [+ -]             2.3380\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4182 - Loss Mean: 23.3336 ----- [+ -]         1.8778\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4182 -- Loss: 27.2267 ----- [+ -]             1.6343\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4183 - Loss Mean: 23.5073 ----- [+ -]         1.8436\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4183 -- Loss: 27.4668 ----- [+ -]             2.4122\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4184 - Loss Mean: 23.4904 ----- [+ -]         2.0164\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4184 -- Loss: 27.2660 ----- [+ -]             2.9856\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4185 - Loss Mean: 23.1504 ----- [+ -]         2.1648\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4185 -- Loss: 26.6490 ----- [+ -]             1.8991\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4186 - Loss Mean: 22.7620 ----- [+ -]         1.8768\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4186 -- Loss: 26.1609 ----- [+ -]             2.4370\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4187 - Loss Mean: 22.3888 ----- [+ -]         2.0192\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4187 -- Loss: 25.9787 ----- [+ -]             1.9414\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4188 - Loss Mean: 22.4735 ----- [+ -]         1.9860\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4188 -- Loss: 25.9071 ----- [+ -]             1.7336\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4189 - Loss Mean: 22.6830 ----- [+ -]         1.7211\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4189 -- Loss: 26.0753 ----- [+ -]             2.1268\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4190 - Loss Mean: 23.2352 ----- [+ -]         2.1260\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4190 -- Loss: 26.2613 ----- [+ -]             2.8400\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4191 - Loss Mean: 23.4653 ----- [+ -]         2.0780\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4191 -- Loss: 26.4553 ----- [+ -]             2.8236\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4192 - Loss Mean: 23.5250 ----- [+ -]         1.8981\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4192 -- Loss: 26.3652 ----- [+ -]             3.2715\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4193 - Loss Mean: 23.4208 ----- [+ -]         1.8119\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4193 -- Loss: 26.4066 ----- [+ -]             2.3300\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4194 - Loss Mean: 23.1107 ----- [+ -]         1.8280\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4194 -- Loss: 26.3058 ----- [+ -]             2.2046\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4195 - Loss Mean: 22.6327 ----- [+ -]         1.5738\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4195 -- Loss: 25.9960 ----- [+ -]             1.9772\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4196 - Loss Mean: 22.3587 ----- [+ -]         1.8349\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4196 -- Loss: 26.0642 ----- [+ -]             1.5778\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4197 - Loss Mean: 22.4113 ----- [+ -]         1.9530\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4197 -- Loss: 26.4591 ----- [+ -]             2.1558\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4198 - Loss Mean: 22.6358 ----- [+ -]         2.2547\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4198 -- Loss: 26.7924 ----- [+ -]             1.9227\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4199 - Loss Mean: 23.0828 ----- [+ -]         1.9456\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4199 -- Loss: 27.2083 ----- [+ -]             2.2611\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4200 - Loss Mean: 23.4649 ----- [+ -]         1.9518\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4200 -- Loss: 27.2294 ----- [+ -]             2.7710\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4201 - Loss Mean: 23.5521 ----- [+ -]         2.1533\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4201 -- Loss: 27.2028 ----- [+ -]             1.7819\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4202 - Loss Mean: 23.2530 ----- [+ -]         1.5855\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4202 -- Loss: 26.9231 ----- [+ -]             2.8204\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4203 - Loss Mean: 22.8881 ----- [+ -]         1.9129\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4203 -- Loss: 26.3969 ----- [+ -]             2.1448\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4204 - Loss Mean: 22.4146 ----- [+ -]         2.1705\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4204 -- Loss: 26.2192 ----- [+ -]             2.3718\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4205 - Loss Mean: 22.2566 ----- [+ -]         2.0052\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4205 -- Loss: 26.0249 ----- [+ -]             2.2977\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4206 - Loss Mean: 22.4632 ----- [+ -]         1.8781\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4206 -- Loss: 26.0394 ----- [+ -]             2.1300\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4207 - Loss Mean: 22.8914 ----- [+ -]         1.5973\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4207 -- Loss: 26.2628 ----- [+ -]             1.3644\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4208 - Loss Mean: 23.2409 ----- [+ -]         1.7161\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4208 -- Loss: 26.4444 ----- [+ -]             1.9107\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4209 - Loss Mean: 23.5151 ----- [+ -]         1.8187\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4209 -- Loss: 26.6012 ----- [+ -]             2.0308\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4210 - Loss Mean: 23.4847 ----- [+ -]         1.8962\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4210 -- Loss: 26.5048 ----- [+ -]             1.6599\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4211 - Loss Mean: 23.2986 ----- [+ -]         1.5779\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4211 -- Loss: 26.3703 ----- [+ -]             1.6502\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4212 - Loss Mean: 22.9362 ----- [+ -]         1.6866\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4212 -- Loss: 25.8393 ----- [+ -]             2.4011\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4213 - Loss Mean: 22.5109 ----- [+ -]         1.5736\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4213 -- Loss: 25.9839 ----- [+ -]             2.4032\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4214 - Loss Mean: 22.3026 ----- [+ -]         1.8853\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4214 -- Loss: 26.1868 ----- [+ -]             2.2977\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4215 - Loss Mean: 22.4821 ----- [+ -]         1.8587\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4215 -- Loss: 26.5938 ----- [+ -]             2.0375\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4216 - Loss Mean: 22.9481 ----- [+ -]         1.5581\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4216 -- Loss: 26.9744 ----- [+ -]             2.2682\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4217 - Loss Mean: 23.2847 ----- [+ -]         1.8240\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4217 -- Loss: 27.2909 ----- [+ -]             1.9673\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4218 - Loss Mean: 23.5095 ----- [+ -]         2.0976\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4218 -- Loss: 27.4327 ----- [+ -]             2.3525\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4219 - Loss Mean: 23.5431 ----- [+ -]         2.1380\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4219 -- Loss: 27.0311 ----- [+ -]             2.7898\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4220 - Loss Mean: 23.0733 ----- [+ -]         2.3640\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4220 -- Loss: 26.7340 ----- [+ -]             1.9683\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4221 - Loss Mean: 22.7072 ----- [+ -]         1.8187\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4221 -- Loss: 26.1912 ----- [+ -]             2.4129\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4222 - Loss Mean: 22.3916 ----- [+ -]         1.8812\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4222 -- Loss: 26.0421 ----- [+ -]             2.1876\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4223 - Loss Mean: 22.2983 ----- [+ -]         1.8714\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4223 -- Loss: 25.9449 ----- [+ -]             2.0523\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4224 - Loss Mean: 22.5712 ----- [+ -]         1.7163\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4224 -- Loss: 26.0812 ----- [+ -]             1.8984\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4225 - Loss Mean: 22.9913 ----- [+ -]         1.8359\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4225 -- Loss: 26.2513 ----- [+ -]             2.4532\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4226 - Loss Mean: 23.2981 ----- [+ -]         2.2854\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4226 -- Loss: 26.5350 ----- [+ -]             2.2243\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4227 - Loss Mean: 23.5313 ----- [+ -]         1.9048\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4227 -- Loss: 26.4783 ----- [+ -]             2.0861\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4228 - Loss Mean: 23.4359 ----- [+ -]         2.0896\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4228 -- Loss: 26.3638 ----- [+ -]             2.0350\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4229 - Loss Mean: 23.1359 ----- [+ -]         1.8428\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4229 -- Loss: 26.2290 ----- [+ -]             2.6156\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4230 - Loss Mean: 22.7469 ----- [+ -]         1.9817\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4230 -- Loss: 25.9932 ----- [+ -]             1.6041\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4231 - Loss Mean: 22.2708 ----- [+ -]         2.3139\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4231 -- Loss: 25.9094 ----- [+ -]             2.7626\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4232 - Loss Mean: 22.2674 ----- [+ -]         1.9595\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4232 -- Loss: 26.1456 ----- [+ -]             1.1492\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4233 - Loss Mean: 22.5615 ----- [+ -]         1.7009\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4233 -- Loss: 26.6232 ----- [+ -]             2.7525\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4234 - Loss Mean: 22.9245 ----- [+ -]         1.7322\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4234 -- Loss: 26.9473 ----- [+ -]             1.7541\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4235 - Loss Mean: 23.5270 ----- [+ -]         2.5687\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4235 -- Loss: 27.2167 ----- [+ -]             2.7690\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4236 - Loss Mean: 23.4506 ----- [+ -]         1.8701\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4236 -- Loss: 27.2863 ----- [+ -]             2.6849\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4237 - Loss Mean: 23.4708 ----- [+ -]         2.1016\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4237 -- Loss: 26.9379 ----- [+ -]             1.8620\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4238 - Loss Mean: 23.0553 ----- [+ -]         1.5360\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4238 -- Loss: 26.4645 ----- [+ -]             2.4581\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4239 - Loss Mean: 22.5892 ----- [+ -]         1.8766\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4239 -- Loss: 26.4819 ----- [+ -]             2.7994\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4240 - Loss Mean: 22.2799 ----- [+ -]         2.0245\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4240 -- Loss: 26.0749 ----- [+ -]             2.8652\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4241 - Loss Mean: 22.3623 ----- [+ -]         1.5474\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4241 -- Loss: 25.9804 ----- [+ -]             1.5216\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4242 - Loss Mean: 22.7193 ----- [+ -]         2.0556\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4242 -- Loss: 26.0228 ----- [+ -]             2.7444\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4243 - Loss Mean: 23.0449 ----- [+ -]         1.7873\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4243 -- Loss: 26.4661 ----- [+ -]             2.5037\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4244 - Loss Mean: 23.3958 ----- [+ -]         2.1703\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4244 -- Loss: 26.4653 ----- [+ -]             2.0343\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4245 - Loss Mean: 23.5689 ----- [+ -]         2.0248\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4245 -- Loss: 26.5815 ----- [+ -]             2.1186\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4246 - Loss Mean: 23.4675 ----- [+ -]         2.1365\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4246 -- Loss: 26.4064 ----- [+ -]             2.3821\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4247 - Loss Mean: 23.1271 ----- [+ -]         1.9807\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4247 -- Loss: 26.1048 ----- [+ -]             1.7317\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4248 - Loss Mean: 22.6660 ----- [+ -]         1.6098\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4248 -- Loss: 25.9560 ----- [+ -]             1.8966\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4249 - Loss Mean: 22.3275 ----- [+ -]         1.7614\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4249 -- Loss: 26.0200 ----- [+ -]             2.7937\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4250 - Loss Mean: 22.2466 ----- [+ -]         2.0176\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4250 -- Loss: 26.1928 ----- [+ -]             1.7576\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4251 - Loss Mean: 22.6668 ----- [+ -]         2.3054\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4251 -- Loss: 26.6248 ----- [+ -]             2.7517\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4252 - Loss Mean: 23.0370 ----- [+ -]         1.9710\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4252 -- Loss: 26.9463 ----- [+ -]             1.4316\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4253 - Loss Mean: 23.3496 ----- [+ -]         1.3522\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4253 -- Loss: 27.3299 ----- [+ -]             2.0766\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4254 - Loss Mean: 23.5160 ----- [+ -]         1.7193\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4254 -- Loss: 27.2075 ----- [+ -]             2.2332\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4255 - Loss Mean: 23.3718 ----- [+ -]         2.0084\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4255 -- Loss: 26.9616 ----- [+ -]             2.6917\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4256 - Loss Mean: 23.0600 ----- [+ -]         1.8191\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4256 -- Loss: 26.7135 ----- [+ -]             1.3171\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4257 - Loss Mean: 22.5259 ----- [+ -]         2.0852\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4257 -- Loss: 26.3384 ----- [+ -]             1.7680\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4258 - Loss Mean: 22.3355 ----- [+ -]         2.1058\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4258 -- Loss: 25.9717 ----- [+ -]             1.5420\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4259 - Loss Mean: 22.3002 ----- [+ -]         1.6589\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4259 -- Loss: 25.9569 ----- [+ -]             2.4844\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4260 - Loss Mean: 22.5096 ----- [+ -]         1.8365\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4260 -- Loss: 26.0583 ----- [+ -]             1.9198\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4261 - Loss Mean: 22.9947 ----- [+ -]         2.3778\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4261 -- Loss: 26.4176 ----- [+ -]             1.9664\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4262 - Loss Mean: 23.3149 ----- [+ -]         1.9884\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4262 -- Loss: 26.5111 ----- [+ -]             2.1245\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4263 - Loss Mean: 23.3931 ----- [+ -]         1.7625\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4263 -- Loss: 26.4231 ----- [+ -]             2.1788\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4264 - Loss Mean: 23.3504 ----- [+ -]         1.8990\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4264 -- Loss: 26.2268 ----- [+ -]             2.5975\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4265 - Loss Mean: 23.0758 ----- [+ -]         1.6287\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4265 -- Loss: 26.1418 ----- [+ -]             1.9808\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4266 - Loss Mean: 22.6307 ----- [+ -]         1.9621\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4266 -- Loss: 25.9088 ----- [+ -]             1.4942\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4267 - Loss Mean: 22.3105 ----- [+ -]         1.9777\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4267 -- Loss: 25.8553 ----- [+ -]             2.5159\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4268 - Loss Mean: 22.3354 ----- [+ -]         1.9434\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4268 -- Loss: 26.2409 ----- [+ -]             2.3279\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4269 - Loss Mean: 22.5299 ----- [+ -]         1.9334\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4269 -- Loss: 26.5604 ----- [+ -]             2.3835\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4270 - Loss Mean: 22.9365 ----- [+ -]         2.1881\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4270 -- Loss: 26.9869 ----- [+ -]             1.2460\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4271 - Loss Mean: 23.2736 ----- [+ -]         2.1264\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4271 -- Loss: 27.0547 ----- [+ -]             2.0285\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4272 - Loss Mean: 23.3991 ----- [+ -]         1.9970\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4272 -- Loss: 27.0875 ----- [+ -]             1.7529\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4273 - Loss Mean: 23.2724 ----- [+ -]         2.1519\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4273 -- Loss: 26.9654 ----- [+ -]             1.8982\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4274 - Loss Mean: 23.0466 ----- [+ -]         2.0554\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4274 -- Loss: 26.7187 ----- [+ -]             2.1950\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4275 - Loss Mean: 22.6183 ----- [+ -]         1.8927\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4275 -- Loss: 26.2929 ----- [+ -]             2.1992\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4276 - Loss Mean: 22.2587 ----- [+ -]         2.0840\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4276 -- Loss: 25.9816 ----- [+ -]             1.3974\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4277 - Loss Mean: 22.1594 ----- [+ -]         2.5481\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4277 -- Loss: 25.7795 ----- [+ -]             2.0193\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4278 - Loss Mean: 22.4192 ----- [+ -]         2.0646\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4278 -- Loss: 26.1163 ----- [+ -]             2.4094\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4279 - Loss Mean: 22.8864 ----- [+ -]         1.8671\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4279 -- Loss: 26.2779 ----- [+ -]             2.0682\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4280 - Loss Mean: 23.1672 ----- [+ -]         2.0465\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4280 -- Loss: 26.5069 ----- [+ -]             2.0978\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4281 - Loss Mean: 23.3510 ----- [+ -]         2.0486\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4281 -- Loss: 26.4131 ----- [+ -]             1.7754\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4282 - Loss Mean: 23.3087 ----- [+ -]         1.9492\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4282 -- Loss: 26.4361 ----- [+ -]             2.2966\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4283 - Loss Mean: 23.0457 ----- [+ -]         1.7825\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4283 -- Loss: 26.1747 ----- [+ -]             2.2365\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4284 - Loss Mean: 22.6454 ----- [+ -]         1.7662\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4284 -- Loss: 25.9273 ----- [+ -]             1.6546\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4285 - Loss Mean: 22.3803 ----- [+ -]         1.6707\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4285 -- Loss: 25.8910 ----- [+ -]             2.0520\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4286 - Loss Mean: 22.2724 ----- [+ -]         2.2345\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4286 -- Loss: 26.0955 ----- [+ -]             1.9179\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4287 - Loss Mean: 22.4734 ----- [+ -]         1.5676\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4287 -- Loss: 26.2926 ----- [+ -]             1.9859\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4288 - Loss Mean: 22.8089 ----- [+ -]         1.7532\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4288 -- Loss: 26.9015 ----- [+ -]             2.2777\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4289 - Loss Mean: 23.0847 ----- [+ -]         1.7230\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4289 -- Loss: 26.9781 ----- [+ -]             2.2378\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4290 - Loss Mean: 23.3666 ----- [+ -]         1.9144\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4290 -- Loss: 27.1887 ----- [+ -]             2.2616\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4291 - Loss Mean: 23.4519 ----- [+ -]         2.3918\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4291 -- Loss: 27.0149 ----- [+ -]             2.3929\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4292 - Loss Mean: 22.9818 ----- [+ -]         1.8900\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4292 -- Loss: 26.5796 ----- [+ -]             3.0811\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4293 - Loss Mean: 22.6568 ----- [+ -]         1.5405\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4293 -- Loss: 26.4375 ----- [+ -]             2.6016\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4294 - Loss Mean: 22.3521 ----- [+ -]         1.9417\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4294 -- Loss: 26.0957 ----- [+ -]             1.7843\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4295 - Loss Mean: 22.2030 ----- [+ -]         2.0802\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4295 -- Loss: 25.9126 ----- [+ -]             2.8489\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4296 - Loss Mean: 22.4259 ----- [+ -]         2.1686\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4296 -- Loss: 25.9373 ----- [+ -]             2.2075\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4297 - Loss Mean: 22.7147 ----- [+ -]         1.8826\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4297 -- Loss: 26.4467 ----- [+ -]             3.1688\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4298 - Loss Mean: 23.0874 ----- [+ -]         1.7527\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4298 -- Loss: 26.4443 ----- [+ -]             1.7365\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4299 - Loss Mean: 23.3457 ----- [+ -]         1.9425\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4299 -- Loss: 26.5160 ----- [+ -]             2.0576\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4300 - Loss Mean: 23.3226 ----- [+ -]         1.5769\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4300 -- Loss: 26.4095 ----- [+ -]             1.9581\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4301 - Loss Mean: 23.0886 ----- [+ -]         1.6788\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4301 -- Loss: 26.0013 ----- [+ -]             2.9223\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4302 - Loss Mean: 22.7065 ----- [+ -]         1.7154\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4302 -- Loss: 26.1531 ----- [+ -]             2.4983\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4303 - Loss Mean: 22.3443 ----- [+ -]         2.1610\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4303 -- Loss: 26.0095 ----- [+ -]             2.2608\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4304 - Loss Mean: 22.2177 ----- [+ -]         1.7541\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4304 -- Loss: 26.0550 ----- [+ -]             2.4231\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4305 - Loss Mean: 22.3237 ----- [+ -]         1.8430\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4305 -- Loss: 26.3364 ----- [+ -]             1.8989\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4306 - Loss Mean: 22.6520 ----- [+ -]         1.9054\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4306 -- Loss: 26.6944 ----- [+ -]             2.2728\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4307 - Loss Mean: 22.9779 ----- [+ -]         1.8917\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4307 -- Loss: 26.9021 ----- [+ -]             2.3533\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4308 - Loss Mean: 23.2121 ----- [+ -]         1.7335\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4308 -- Loss: 26.9823 ----- [+ -]             2.1274\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4309 - Loss Mean: 23.2662 ----- [+ -]         1.9962\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4309 -- Loss: 27.1412 ----- [+ -]             2.4465\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4310 - Loss Mean: 23.0557 ----- [+ -]         2.0162\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4310 -- Loss: 26.7397 ----- [+ -]             3.1943\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4311 - Loss Mean: 22.7516 ----- [+ -]         1.8449\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4311 -- Loss: 26.3146 ----- [+ -]             1.8876\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4312 - Loss Mean: 22.3740 ----- [+ -]         1.6854\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4312 -- Loss: 25.8985 ----- [+ -]             2.0261\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4313 - Loss Mean: 22.1810 ----- [+ -]         1.9720\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4313 -- Loss: 25.8685 ----- [+ -]             2.3702\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4314 - Loss Mean: 22.3926 ----- [+ -]         1.8151\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4314 -- Loss: 26.0652 ----- [+ -]             1.6990\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4315 - Loss Mean: 22.6063 ----- [+ -]         1.5498\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4315 -- Loss: 26.1700 ----- [+ -]             1.7620\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4316 - Loss Mean: 22.9640 ----- [+ -]         1.8145\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4316 -- Loss: 26.2964 ----- [+ -]             2.4829\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4317 - Loss Mean: 23.1348 ----- [+ -]         2.1612\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4317 -- Loss: 26.3390 ----- [+ -]             1.9378\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4318 - Loss Mean: 23.2125 ----- [+ -]         1.7356\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4318 -- Loss: 26.4547 ----- [+ -]             2.5749\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4319 - Loss Mean: 22.9683 ----- [+ -]         1.7439\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4319 -- Loss: 26.2600 ----- [+ -]             1.8468\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4320 - Loss Mean: 22.6794 ----- [+ -]         2.1061\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4320 -- Loss: 26.2715 ----- [+ -]             3.0105\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4321 - Loss Mean: 22.3289 ----- [+ -]         1.8999\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4321 -- Loss: 25.8414 ----- [+ -]             2.2051\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4322 - Loss Mean: 22.1345 ----- [+ -]         1.8538\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4322 -- Loss: 26.0299 ----- [+ -]             2.4160\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4323 - Loss Mean: 22.3379 ----- [+ -]         1.6859\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4323 -- Loss: 26.1735 ----- [+ -]             2.1615\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4324 - Loss Mean: 22.5060 ----- [+ -]         1.8817\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4324 -- Loss: 26.5140 ----- [+ -]             2.0508\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4325 - Loss Mean: 22.7712 ----- [+ -]         2.0767\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4325 -- Loss: 26.7646 ----- [+ -]             3.1748\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4326 - Loss Mean: 23.1752 ----- [+ -]         2.4682\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4326 -- Loss: 27.1314 ----- [+ -]             2.2800\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4327 - Loss Mean: 23.1319 ----- [+ -]         1.8930\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4327 -- Loss: 26.9041 ----- [+ -]             2.1097\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4328 - Loss Mean: 23.0104 ----- [+ -]         1.9098\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4328 -- Loss: 26.7186 ----- [+ -]             2.5971\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4329 - Loss Mean: 22.6026 ----- [+ -]         2.3683\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4329 -- Loss: 26.4637 ----- [+ -]             2.4021\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4330 - Loss Mean: 22.4666 ----- [+ -]         1.9258\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4330 -- Loss: 25.9816 ----- [+ -]             2.4440\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4331 - Loss Mean: 22.2799 ----- [+ -]         1.9664\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4331 -- Loss: 25.8354 ----- [+ -]             2.3541\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4332 - Loss Mean: 22.2708 ----- [+ -]         1.8024\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4332 -- Loss: 25.8814 ----- [+ -]             2.1373\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4333 - Loss Mean: 22.5493 ----- [+ -]         2.0250\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4333 -- Loss: 26.1183 ----- [+ -]             1.6866\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4334 - Loss Mean: 22.8039 ----- [+ -]         1.7556\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4334 -- Loss: 26.1456 ----- [+ -]             2.3572\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4335 - Loss Mean: 23.0876 ----- [+ -]         2.0128\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4335 -- Loss: 26.3376 ----- [+ -]             1.7710\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4336 - Loss Mean: 23.0871 ----- [+ -]         1.5035\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4336 -- Loss: 26.2460 ----- [+ -]             1.9564\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4337 - Loss Mean: 22.9660 ----- [+ -]         1.7033\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4337 -- Loss: 26.1077 ----- [+ -]             2.0944\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4338 - Loss Mean: 22.6682 ----- [+ -]         1.9342\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4338 -- Loss: 26.0880 ----- [+ -]             1.9895\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4339 - Loss Mean: 22.4139 ----- [+ -]         1.7065\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4339 -- Loss: 25.9405 ----- [+ -]             2.3642\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4340 - Loss Mean: 22.2095 ----- [+ -]         1.5243\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4340 -- Loss: 26.0117 ----- [+ -]             1.9848\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4341 - Loss Mean: 22.3389 ----- [+ -]         1.9434\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4341 -- Loss: 26.1332 ----- [+ -]             2.1183\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4342 - Loss Mean: 22.5126 ----- [+ -]         1.7893\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4342 -- Loss: 26.4953 ----- [+ -]             1.9590\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4343 - Loss Mean: 22.8135 ----- [+ -]         1.7530\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4343 -- Loss: 26.8129 ----- [+ -]             2.1584\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4344 - Loss Mean: 22.9828 ----- [+ -]         1.8852\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4344 -- Loss: 26.8174 ----- [+ -]             2.1918\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4345 - Loss Mean: 23.1018 ----- [+ -]         1.9560\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4345 -- Loss: 26.9399 ----- [+ -]             2.3318\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4346 - Loss Mean: 22.9563 ----- [+ -]         1.9026\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4346 -- Loss: 26.5728 ----- [+ -]             1.9912\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4347 - Loss Mean: 22.7697 ----- [+ -]         1.9112\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4347 -- Loss: 26.4288 ----- [+ -]             1.8071\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4348 - Loss Mean: 22.4187 ----- [+ -]         1.7965\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4348 -- Loss: 25.9755 ----- [+ -]             1.8130\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4349 - Loss Mean: 22.2350 ----- [+ -]         1.9214\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4349 -- Loss: 25.9479 ----- [+ -]             1.8246\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4350 - Loss Mean: 22.3673 ----- [+ -]         2.2515\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4350 -- Loss: 25.8304 ----- [+ -]             2.4931\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4351 - Loss Mean: 22.4536 ----- [+ -]         1.8763\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4351 -- Loss: 25.9983 ----- [+ -]             2.4815\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4352 - Loss Mean: 22.7658 ----- [+ -]         1.8017\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4352 -- Loss: 26.2194 ----- [+ -]             2.3660\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4353 - Loss Mean: 23.0197 ----- [+ -]         2.0965\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4353 -- Loss: 26.1724 ----- [+ -]             2.3721\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4354 - Loss Mean: 23.0923 ----- [+ -]         1.9054\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4354 -- Loss: 26.2101 ----- [+ -]             2.7256\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4355 - Loss Mean: 22.8640 ----- [+ -]         2.2730\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4355 -- Loss: 26.1026 ----- [+ -]             2.6720\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4356 - Loss Mean: 22.6078 ----- [+ -]         1.8376\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4356 -- Loss: 26.0306 ----- [+ -]             1.7915\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4357 - Loss Mean: 22.4767 ----- [+ -]         2.1006\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4357 -- Loss: 25.7613 ----- [+ -]             2.5986\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4358 - Loss Mean: 22.2494 ----- [+ -]         1.8368\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4358 -- Loss: 26.1631 ----- [+ -]             2.5918\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4359 - Loss Mean: 22.2779 ----- [+ -]         2.0469\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4359 -- Loss: 26.1928 ----- [+ -]             1.3337\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4360 - Loss Mean: 22.4574 ----- [+ -]         1.7658\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4360 -- Loss: 26.4366 ----- [+ -]             1.5917\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4361 - Loss Mean: 22.7648 ----- [+ -]         1.7602\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4361 -- Loss: 26.6820 ----- [+ -]             2.2216\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4362 - Loss Mean: 22.9411 ----- [+ -]         2.2458\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4362 -- Loss: 27.0004 ----- [+ -]             2.6292\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4363 - Loss Mean: 23.0594 ----- [+ -]         1.7407\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4363 -- Loss: 26.7212 ----- [+ -]             2.1116\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4364 - Loss Mean: 22.8819 ----- [+ -]         2.0201\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4364 -- Loss: 26.5419 ----- [+ -]             2.8483\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4365 - Loss Mean: 22.6215 ----- [+ -]         1.9806\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4365 -- Loss: 26.3525 ----- [+ -]             1.4204\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4366 - Loss Mean: 22.4351 ----- [+ -]         1.9120\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4366 -- Loss: 25.8463 ----- [+ -]             2.7637\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4367 - Loss Mean: 22.2156 ----- [+ -]         1.9695\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4367 -- Loss: 25.9676 ----- [+ -]             2.1040\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4368 - Loss Mean: 22.2445 ----- [+ -]         1.9014\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4368 -- Loss: 26.0588 ----- [+ -]             2.7677\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4369 - Loss Mean: 22.4943 ----- [+ -]         1.9809\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4369 -- Loss: 26.0312 ----- [+ -]             2.1836\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4370 - Loss Mean: 22.8786 ----- [+ -]         2.0599\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4370 -- Loss: 26.1428 ----- [+ -]             2.6961\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4371 - Loss Mean: 23.0519 ----- [+ -]         1.9904\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4371 -- Loss: 26.2871 ----- [+ -]             2.0676\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4372 - Loss Mean: 23.0921 ----- [+ -]         2.0580\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4372 -- Loss: 26.1583 ----- [+ -]             1.7863\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4373 - Loss Mean: 22.8427 ----- [+ -]         2.0458\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4373 -- Loss: 26.1804 ----- [+ -]             2.1638\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4374 - Loss Mean: 22.7032 ----- [+ -]         1.9277\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4374 -- Loss: 26.0509 ----- [+ -]             2.2959\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4375 - Loss Mean: 22.3483 ----- [+ -]         1.9176\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4375 -- Loss: 25.8973 ----- [+ -]             1.7646\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4376 - Loss Mean: 22.2131 ----- [+ -]         2.0729\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4376 -- Loss: 25.9871 ----- [+ -]             2.0774\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4377 - Loss Mean: 22.2743 ----- [+ -]         2.3470\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4377 -- Loss: 26.2011 ----- [+ -]             2.3640\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4378 - Loss Mean: 22.5471 ----- [+ -]         1.9144\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4378 -- Loss: 26.5029 ----- [+ -]             2.4934\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4379 - Loss Mean: 22.8023 ----- [+ -]         1.8257\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4379 -- Loss: 26.7033 ----- [+ -]             2.0218\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4380 - Loss Mean: 22.9657 ----- [+ -]         1.7910\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4380 -- Loss: 26.6439 ----- [+ -]             3.0177\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4381 - Loss Mean: 22.9838 ----- [+ -]         1.8461\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4381 -- Loss: 26.6067 ----- [+ -]             2.4092\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4382 - Loss Mean: 22.8803 ----- [+ -]         1.7466\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4382 -- Loss: 26.6383 ----- [+ -]             2.2510\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4383 - Loss Mean: 22.5732 ----- [+ -]         2.0853\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4383 -- Loss: 26.1540 ----- [+ -]             2.0682\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4384 - Loss Mean: 22.3564 ----- [+ -]         2.0276\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4384 -- Loss: 26.1867 ----- [+ -]             2.9295\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4385 - Loss Mean: 22.2351 ----- [+ -]         1.9661\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4385 -- Loss: 25.7502 ----- [+ -]             2.8447\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4386 - Loss Mean: 22.2626 ----- [+ -]         2.0356\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4386 -- Loss: 25.8241 ----- [+ -]             2.0262\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4387 - Loss Mean: 22.4750 ----- [+ -]         2.0039\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4387 -- Loss: 26.0374 ----- [+ -]             2.6591\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4388 - Loss Mean: 22.7460 ----- [+ -]         1.4635\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4388 -- Loss: 26.2765 ----- [+ -]             2.1907\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4389 - Loss Mean: 22.9436 ----- [+ -]         1.6627\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4389 -- Loss: 26.2164 ----- [+ -]             1.7593\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4390 - Loss Mean: 23.0089 ----- [+ -]         1.8991\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4390 -- Loss: 26.2107 ----- [+ -]             2.3073\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4391 - Loss Mean: 22.9105 ----- [+ -]         1.8282\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4391 -- Loss: 26.0271 ----- [+ -]             1.6384\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4392 - Loss Mean: 22.5350 ----- [+ -]         1.7676\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4392 -- Loss: 25.9845 ----- [+ -]             2.4310\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4393 - Loss Mean: 22.3598 ----- [+ -]         1.7165\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4393 -- Loss: 25.8295 ----- [+ -]             1.7541\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4394 - Loss Mean: 22.3068 ----- [+ -]         2.2289\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4394 -- Loss: 26.0410 ----- [+ -]             2.6503\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4395 - Loss Mean: 22.2692 ----- [+ -]         1.9341\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4395 -- Loss: 26.1141 ----- [+ -]             2.1675\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4396 - Loss Mean: 22.5284 ----- [+ -]         2.1073\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4396 -- Loss: 26.5855 ----- [+ -]             2.3340\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4397 - Loss Mean: 22.7412 ----- [+ -]         1.9336\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4397 -- Loss: 26.7905 ----- [+ -]             3.1655\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4398 - Loss Mean: 22.8693 ----- [+ -]         1.8109\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4398 -- Loss: 26.8588 ----- [+ -]             1.3433\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4399 - Loss Mean: 22.8816 ----- [+ -]         2.0963\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4399 -- Loss: 26.6838 ----- [+ -]             1.4496\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4400 - Loss Mean: 22.7382 ----- [+ -]         2.1471\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4400 -- Loss: 26.4138 ----- [+ -]             1.7934\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4401 - Loss Mean: 22.4644 ----- [+ -]         2.2724\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4401 -- Loss: 26.1726 ----- [+ -]             1.7384\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4402 - Loss Mean: 22.3058 ----- [+ -]         1.7368\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4402 -- Loss: 25.9877 ----- [+ -]             1.4010\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4403 - Loss Mean: 22.2295 ----- [+ -]         1.9717\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4403 -- Loss: 26.0519 ----- [+ -]             2.5908\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4404 - Loss Mean: 22.2716 ----- [+ -]         1.9044\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4404 -- Loss: 25.9956 ----- [+ -]             2.1138\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4405 - Loss Mean: 22.5617 ----- [+ -]         1.6611\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4405 -- Loss: 26.2202 ----- [+ -]             2.2031\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4406 - Loss Mean: 22.7525 ----- [+ -]         1.9072\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4406 -- Loss: 26.2463 ----- [+ -]             2.3296\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4407 - Loss Mean: 22.9018 ----- [+ -]         2.3953\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4407 -- Loss: 26.1302 ----- [+ -]             2.0848\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4408 - Loss Mean: 22.9089 ----- [+ -]         1.5337\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4408 -- Loss: 26.0000 ----- [+ -]             2.1209\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4409 - Loss Mean: 22.7068 ----- [+ -]         2.2255\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4409 -- Loss: 26.0409 ----- [+ -]             2.3362\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4410 - Loss Mean: 22.4393 ----- [+ -]         1.8424\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4410 -- Loss: 25.8127 ----- [+ -]             1.7712\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4411 - Loss Mean: 22.2409 ----- [+ -]         1.7927\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4411 -- Loss: 25.9343 ----- [+ -]             2.3028\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4412 - Loss Mean: 22.1830 ----- [+ -]         1.9228\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4412 -- Loss: 25.9250 ----- [+ -]             2.3322\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4413 - Loss Mean: 22.3096 ----- [+ -]         1.6907\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4413 -- Loss: 26.0337 ----- [+ -]             2.4805\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4414 - Loss Mean: 22.5727 ----- [+ -]         1.7946\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4414 -- Loss: 26.4558 ----- [+ -]             2.0365\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4415 - Loss Mean: 22.8669 ----- [+ -]         2.1899\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4415 -- Loss: 26.5175 ----- [+ -]             2.4492\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4416 - Loss Mean: 22.9033 ----- [+ -]         2.2365\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4416 -- Loss: 26.7862 ----- [+ -]             1.8576\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4417 - Loss Mean: 22.8008 ----- [+ -]         2.2435\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4417 -- Loss: 26.5950 ----- [+ -]             2.1144\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4418 - Loss Mean: 22.6182 ----- [+ -]         2.1318\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4418 -- Loss: 26.4306 ----- [+ -]             2.0134\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4419 - Loss Mean: 22.4475 ----- [+ -]         2.0634\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4419 -- Loss: 26.0256 ----- [+ -]             2.7970\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4420 - Loss Mean: 22.1986 ----- [+ -]         1.7336\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4420 -- Loss: 25.8292 ----- [+ -]             1.8823\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4421 - Loss Mean: 22.1966 ----- [+ -]         1.6919\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4421 -- Loss: 25.9894 ----- [+ -]             2.1341\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4422 - Loss Mean: 22.3963 ----- [+ -]         2.0924\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4422 -- Loss: 26.0221 ----- [+ -]             2.2104\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4423 - Loss Mean: 22.6109 ----- [+ -]         1.7648\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4423 -- Loss: 26.0314 ----- [+ -]             2.0516\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4424 - Loss Mean: 22.8396 ----- [+ -]         2.0730\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4424 -- Loss: 25.9972 ----- [+ -]             2.4764\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4425 - Loss Mean: 22.8431 ----- [+ -]         2.1997\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4425 -- Loss: 26.2270 ----- [+ -]             1.6621\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4426 - Loss Mean: 22.8715 ----- [+ -]         1.8597\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4426 -- Loss: 26.1577 ----- [+ -]             1.7313\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4427 - Loss Mean: 22.7660 ----- [+ -]         2.5345\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4427 -- Loss: 25.8543 ----- [+ -]             3.0053\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4428 - Loss Mean: 22.3944 ----- [+ -]         1.9020\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4428 -- Loss: 25.8869 ----- [+ -]             2.1272\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4429 - Loss Mean: 22.1638 ----- [+ -]         1.9829\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4429 -- Loss: 25.9469 ----- [+ -]             1.4968\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4430 - Loss Mean: 22.2044 ----- [+ -]         1.8778\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4430 -- Loss: 26.0639 ----- [+ -]             3.0655\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4431 - Loss Mean: 22.4900 ----- [+ -]         2.3305\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4431 -- Loss: 26.3878 ----- [+ -]             2.2579\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4432 - Loss Mean: 22.5358 ----- [+ -]         2.0452\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4432 -- Loss: 26.4370 ----- [+ -]             2.1593\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4433 - Loss Mean: 22.7203 ----- [+ -]         2.0352\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4433 -- Loss: 26.7179 ----- [+ -]             2.5979\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4434 - Loss Mean: 22.8608 ----- [+ -]         2.1892\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4434 -- Loss: 26.7356 ----- [+ -]             2.3702\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4435 - Loss Mean: 22.8267 ----- [+ -]         1.8803\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4435 -- Loss: 26.4659 ----- [+ -]             1.4495\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4436 - Loss Mean: 22.5555 ----- [+ -]         1.9071\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4436 -- Loss: 26.3122 ----- [+ -]             1.5351\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4437 - Loss Mean: 22.3874 ----- [+ -]         2.0475\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4437 -- Loss: 25.8795 ----- [+ -]             1.8992\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4438 - Loss Mean: 22.1929 ----- [+ -]         1.9217\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4438 -- Loss: 25.8743 ----- [+ -]             1.7816\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4439 - Loss Mean: 22.2631 ----- [+ -]         1.8598\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4439 -- Loss: 25.8498 ----- [+ -]             1.8830\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4440 - Loss Mean: 22.5282 ----- [+ -]         2.0498\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4440 -- Loss: 25.9699 ----- [+ -]             2.1641\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4441 - Loss Mean: 22.6857 ----- [+ -]         2.2004\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4441 -- Loss: 26.0713 ----- [+ -]             2.6061\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4442 - Loss Mean: 22.7960 ----- [+ -]         1.7371\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4442 -- Loss: 26.1737 ----- [+ -]             1.5437\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4443 - Loss Mean: 22.9661 ----- [+ -]         2.2791\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4443 -- Loss: 26.2046 ----- [+ -]             2.2892\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4444 - Loss Mean: 22.7325 ----- [+ -]         1.5472\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4444 -- Loss: 25.9603 ----- [+ -]             1.7846\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4445 - Loss Mean: 22.4481 ----- [+ -]         2.0018\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4445 -- Loss: 25.9410 ----- [+ -]             2.3668\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4446 - Loss Mean: 22.2487 ----- [+ -]         1.8873\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4446 -- Loss: 25.9625 ----- [+ -]             1.7336\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4447 - Loss Mean: 22.1997 ----- [+ -]         1.8735\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4447 -- Loss: 26.0178 ----- [+ -]             2.1667\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4448 - Loss Mean: 22.2247 ----- [+ -]         1.9857\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4448 -- Loss: 26.0848 ----- [+ -]             1.1453\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4449 - Loss Mean: 22.4093 ----- [+ -]         1.8515\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4449 -- Loss: 26.5825 ----- [+ -]             3.2231\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4450 - Loss Mean: 22.6330 ----- [+ -]         1.8980\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4450 -- Loss: 26.5280 ----- [+ -]             1.8384\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4451 - Loss Mean: 22.7508 ----- [+ -]         1.7830\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4451 -- Loss: 26.7050 ----- [+ -]             2.2753\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4452 - Loss Mean: 22.7771 ----- [+ -]         1.7700\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4452 -- Loss: 26.6626 ----- [+ -]             2.4692\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4453 - Loss Mean: 22.6255 ----- [+ -]         1.7872\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4453 -- Loss: 26.4193 ----- [+ -]             2.0715\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4454 - Loss Mean: 22.3763 ----- [+ -]         1.8842\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4454 -- Loss: 26.1367 ----- [+ -]             1.1913\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4455 - Loss Mean: 22.2435 ----- [+ -]         1.9756\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4455 -- Loss: 25.8962 ----- [+ -]             2.5151\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4456 - Loss Mean: 22.2228 ----- [+ -]         2.0395\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4456 -- Loss: 25.8008 ----- [+ -]             2.2369\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4457 - Loss Mean: 22.3082 ----- [+ -]         1.9164\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4457 -- Loss: 25.8949 ----- [+ -]             1.8117\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4458 - Loss Mean: 22.4407 ----- [+ -]         1.9750\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4458 -- Loss: 26.0491 ----- [+ -]             1.4803\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4459 - Loss Mean: 22.7047 ----- [+ -]         1.7605\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4459 -- Loss: 26.1703 ----- [+ -]             2.0142\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4460 - Loss Mean: 22.7749 ----- [+ -]         2.0404\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4460 -- Loss: 26.0691 ----- [+ -]             2.2462\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4461 - Loss Mean: 22.7483 ----- [+ -]         1.9656\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4461 -- Loss: 25.9332 ----- [+ -]             2.8363\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4462 - Loss Mean: 22.6123 ----- [+ -]         1.8431\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4462 -- Loss: 26.0302 ----- [+ -]             1.8345\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4463 - Loss Mean: 22.4185 ----- [+ -]         1.7407\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4463 -- Loss: 25.7847 ----- [+ -]             2.1716\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4464 - Loss Mean: 22.2328 ----- [+ -]         1.8779\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4464 -- Loss: 25.9116 ----- [+ -]             1.9274\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4465 - Loss Mean: 22.1240 ----- [+ -]         2.1548\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4465 -- Loss: 26.0949 ----- [+ -]             2.2333\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4466 - Loss Mean: 22.3053 ----- [+ -]         2.1814\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4466 -- Loss: 26.1912 ----- [+ -]             2.6788\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4467 - Loss Mean: 22.5604 ----- [+ -]         1.6395\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4467 -- Loss: 26.4006 ----- [+ -]             2.4397\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4468 - Loss Mean: 22.6437 ----- [+ -]         2.2835\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4468 -- Loss: 26.6057 ----- [+ -]             2.5370\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4469 - Loss Mean: 22.8185 ----- [+ -]         1.7591\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4469 -- Loss: 26.7313 ----- [+ -]             1.6793\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4470 - Loss Mean: 22.7487 ----- [+ -]         1.8179\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4470 -- Loss: 26.3302 ----- [+ -]             3.0910\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4471 - Loss Mean: 22.6186 ----- [+ -]         2.1842\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4471 -- Loss: 26.2629 ----- [+ -]             2.4766\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4472 - Loss Mean: 22.3874 ----- [+ -]         2.2059\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4472 -- Loss: 26.0332 ----- [+ -]             2.3592\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4473 - Loss Mean: 22.1938 ----- [+ -]         1.5834\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4473 -- Loss: 26.0237 ----- [+ -]             1.9606\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4474 - Loss Mean: 22.2224 ----- [+ -]         1.8796\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4474 -- Loss: 26.0746 ----- [+ -]             2.3173\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4475 - Loss Mean: 22.4583 ----- [+ -]         2.1789\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4475 -- Loss: 26.0586 ----- [+ -]             1.8975\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4476 - Loss Mean: 22.6302 ----- [+ -]         1.7960\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4476 -- Loss: 26.2557 ----- [+ -]             2.4445\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4477 - Loss Mean: 22.7696 ----- [+ -]         1.9100\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4477 -- Loss: 26.0741 ----- [+ -]             2.9325\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4478 - Loss Mean: 22.7884 ----- [+ -]         1.8296\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4478 -- Loss: 26.1119 ----- [+ -]             1.7534\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4479 - Loss Mean: 22.6870 ----- [+ -]         1.8519\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4479 -- Loss: 25.9722 ----- [+ -]             1.7496\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4480 - Loss Mean: 22.5388 ----- [+ -]         1.9593\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4480 -- Loss: 25.8952 ----- [+ -]             2.0501\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4481 - Loss Mean: 22.2689 ----- [+ -]         1.8143\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4481 -- Loss: 25.9350 ----- [+ -]             1.6032\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4482 - Loss Mean: 22.1072 ----- [+ -]         1.9613\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4482 -- Loss: 25.8630 ----- [+ -]             2.4600\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4483 - Loss Mean: 22.2208 ----- [+ -]         1.6039\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4483 -- Loss: 26.1743 ----- [+ -]             1.9492\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4484 - Loss Mean: 22.3978 ----- [+ -]         1.8018\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4484 -- Loss: 26.4522 ----- [+ -]             2.4635\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4485 - Loss Mean: 22.5704 ----- [+ -]         1.8179\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4485 -- Loss: 26.5615 ----- [+ -]             2.3204\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4486 - Loss Mean: 22.7070 ----- [+ -]         1.8580\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4486 -- Loss: 26.5377 ----- [+ -]             2.1027\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4487 - Loss Mean: 22.6882 ----- [+ -]         2.0139\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4487 -- Loss: 26.7202 ----- [+ -]             2.4817\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4488 - Loss Mean: 22.5880 ----- [+ -]         2.0292\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4488 -- Loss: 26.3568 ----- [+ -]             1.5202\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4489 - Loss Mean: 22.3880 ----- [+ -]         2.2205\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4489 -- Loss: 26.0031 ----- [+ -]             2.3460\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4490 - Loss Mean: 22.1903 ----- [+ -]         1.9864\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4490 -- Loss: 25.9659 ----- [+ -]             1.8626\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4491 - Loss Mean: 22.1437 ----- [+ -]         2.1129\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4491 -- Loss: 25.7912 ----- [+ -]             2.3722\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4492 - Loss Mean: 22.3375 ----- [+ -]         1.8755\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4492 -- Loss: 25.8368 ----- [+ -]             2.7918\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4493 - Loss Mean: 22.5048 ----- [+ -]         1.8969\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4493 -- Loss: 26.1327 ----- [+ -]             1.3302\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4494 - Loss Mean: 22.6221 ----- [+ -]         1.9737\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4494 -- Loss: 26.0673 ----- [+ -]             1.5531\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4495 - Loss Mean: 22.7165 ----- [+ -]         1.5942\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4495 -- Loss: 26.0858 ----- [+ -]             1.2601\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4496 - Loss Mean: 22.6533 ----- [+ -]         2.1706\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4496 -- Loss: 25.8764 ----- [+ -]             3.7851\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4497 - Loss Mean: 22.4360 ----- [+ -]         1.8796\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4497 -- Loss: 25.9487 ----- [+ -]             1.8435\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4498 - Loss Mean: 22.3083 ----- [+ -]         1.7405\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4498 -- Loss: 25.7872 ----- [+ -]             2.2415\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4499 - Loss Mean: 22.1897 ----- [+ -]         1.8613\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4499 -- Loss: 26.1261 ----- [+ -]             2.7478\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4500 - Loss Mean: 22.1064 ----- [+ -]         1.9794\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4500 -- Loss: 26.1856 ----- [+ -]             2.4688\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4501 - Loss Mean: 22.2747 ----- [+ -]         1.8854\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4501 -- Loss: 26.3444 ----- [+ -]             2.4435\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4502 - Loss Mean: 22.6058 ----- [+ -]         2.1633\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4502 -- Loss: 26.5312 ----- [+ -]             1.4363\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4503 - Loss Mean: 22.7186 ----- [+ -]         2.2065\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4503 -- Loss: 26.4241 ----- [+ -]             2.1645\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4504 - Loss Mean: 22.7149 ----- [+ -]         1.7576\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4504 -- Loss: 26.7311 ----- [+ -]             2.3037\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4505 - Loss Mean: 22.5984 ----- [+ -]         1.6429\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4505 -- Loss: 26.3094 ----- [+ -]             2.4877\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4506 - Loss Mean: 22.4142 ----- [+ -]         2.0266\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4506 -- Loss: 26.1316 ----- [+ -]             1.9094\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4507 - Loss Mean: 22.2305 ----- [+ -]         1.8508\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4507 -- Loss: 26.0217 ----- [+ -]             2.2859\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4508 - Loss Mean: 22.1584 ----- [+ -]         1.9218\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4508 -- Loss: 26.0050 ----- [+ -]             2.1009\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4509 - Loss Mean: 22.2726 ----- [+ -]         1.8706\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4509 -- Loss: 25.8427 ----- [+ -]             2.7688\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4510 - Loss Mean: 22.4067 ----- [+ -]         1.6818\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4510 -- Loss: 25.9601 ----- [+ -]             2.3409\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4511 - Loss Mean: 22.6835 ----- [+ -]         1.9874\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4511 -- Loss: 26.3194 ----- [+ -]             2.5136\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4512 - Loss Mean: 22.7411 ----- [+ -]         1.6978\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4512 -- Loss: 26.0357 ----- [+ -]             2.1660\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4513 - Loss Mean: 22.6450 ----- [+ -]         2.0543\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4513 -- Loss: 26.0583 ----- [+ -]             2.1881\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4514 - Loss Mean: 22.4941 ----- [+ -]         1.9167\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4514 -- Loss: 26.0841 ----- [+ -]             2.4853\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4515 - Loss Mean: 22.3228 ----- [+ -]         2.1439\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4515 -- Loss: 25.9607 ----- [+ -]             1.8755\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4516 - Loss Mean: 22.2001 ----- [+ -]         1.9339\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4516 -- Loss: 25.9892 ----- [+ -]             1.4879\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4517 - Loss Mean: 22.1845 ----- [+ -]         1.9720\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4517 -- Loss: 26.0825 ----- [+ -]             1.3410\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4518 - Loss Mean: 22.2426 ----- [+ -]         2.1791\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4518 -- Loss: 26.3330 ----- [+ -]             2.7255\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4519 - Loss Mean: 22.4585 ----- [+ -]         2.0579\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4519 -- Loss: 26.3600 ----- [+ -]             2.1807\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4520 - Loss Mean: 22.6252 ----- [+ -]         1.9726\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4520 -- Loss: 26.6372 ----- [+ -]             2.6728\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4521 - Loss Mean: 22.7179 ----- [+ -]         1.8465\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4521 -- Loss: 26.5560 ----- [+ -]             2.0625\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4522 - Loss Mean: 22.5354 ----- [+ -]         2.1024\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4522 -- Loss: 26.4222 ----- [+ -]             2.2276\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4523 - Loss Mean: 22.3365 ----- [+ -]         1.8331\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4523 -- Loss: 26.2143 ----- [+ -]             1.7713\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4524 - Loss Mean: 22.1632 ----- [+ -]         1.6146\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4524 -- Loss: 26.1140 ----- [+ -]             2.3672\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4525 - Loss Mean: 22.1454 ----- [+ -]         2.0783\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4525 -- Loss: 25.9661 ----- [+ -]             1.4530\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4526 - Loss Mean: 22.2909 ----- [+ -]         1.9006\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4526 -- Loss: 25.9222 ----- [+ -]             2.7172\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4527 - Loss Mean: 22.3804 ----- [+ -]         2.0493\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4527 -- Loss: 26.0064 ----- [+ -]             2.2400\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4528 - Loss Mean: 22.6175 ----- [+ -]         2.0829\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4528 -- Loss: 25.9306 ----- [+ -]             3.3971\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4529 - Loss Mean: 22.6079 ----- [+ -]         2.2623\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4529 -- Loss: 26.0721 ----- [+ -]             2.3369\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4530 - Loss Mean: 22.5753 ----- [+ -]         1.5953\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4530 -- Loss: 26.0167 ----- [+ -]             1.6731\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4531 - Loss Mean: 22.4097 ----- [+ -]         1.8001\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4531 -- Loss: 25.8932 ----- [+ -]             2.2581\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4532 - Loss Mean: 22.3356 ----- [+ -]         2.0362\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4532 -- Loss: 25.9753 ----- [+ -]             1.8571\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4533 - Loss Mean: 22.1614 ----- [+ -]         1.9362\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4533 -- Loss: 25.9857 ----- [+ -]             2.3702\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4534 - Loss Mean: 22.1483 ----- [+ -]         1.9175\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4534 -- Loss: 26.0513 ----- [+ -]             1.8944\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4535 - Loss Mean: 22.3042 ----- [+ -]         1.9529\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4535 -- Loss: 26.3343 ----- [+ -]             1.4559\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4536 - Loss Mean: 22.4253 ----- [+ -]         1.9415\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4536 -- Loss: 26.5860 ----- [+ -]             2.1346\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4537 - Loss Mean: 22.5677 ----- [+ -]         1.6637\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4537 -- Loss: 26.5373 ----- [+ -]             2.3144\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4538 - Loss Mean: 22.6251 ----- [+ -]         1.7989\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4538 -- Loss: 26.5144 ----- [+ -]             1.9467\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4539 - Loss Mean: 22.5253 ----- [+ -]         1.9782\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4539 -- Loss: 26.4079 ----- [+ -]             2.8832\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4540 - Loss Mean: 22.2974 ----- [+ -]         1.8676\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4540 -- Loss: 26.2183 ----- [+ -]             2.3369\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4541 - Loss Mean: 22.1520 ----- [+ -]         1.8773\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4541 -- Loss: 25.9027 ----- [+ -]             2.1257\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4542 - Loss Mean: 22.0913 ----- [+ -]         1.8280\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4542 -- Loss: 25.8798 ----- [+ -]             1.8995\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4543 - Loss Mean: 22.2102 ----- [+ -]         2.0889\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4543 -- Loss: 26.1115 ----- [+ -]             2.3682\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4544 - Loss Mean: 22.3625 ----- [+ -]         2.1297\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4544 -- Loss: 26.0044 ----- [+ -]             2.5833\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4545 - Loss Mean: 22.5473 ----- [+ -]         1.8860\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4545 -- Loss: 26.2205 ----- [+ -]             2.4165\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4546 - Loss Mean: 22.7191 ----- [+ -]         1.9018\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4546 -- Loss: 26.0959 ----- [+ -]             1.3689\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4547 - Loss Mean: 22.6713 ----- [+ -]         2.2550\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4547 -- Loss: 26.0904 ----- [+ -]             1.8452\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4548 - Loss Mean: 22.4223 ----- [+ -]         1.8551\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4548 -- Loss: 26.1194 ----- [+ -]             1.6724\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4549 - Loss Mean: 22.2417 ----- [+ -]         1.8269\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4549 -- Loss: 25.8868 ----- [+ -]             2.0460\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4550 - Loss Mean: 22.1993 ----- [+ -]         1.7062\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4550 -- Loss: 25.9894 ----- [+ -]             2.3774\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4551 - Loss Mean: 22.1631 ----- [+ -]         1.9314\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4551 -- Loss: 26.1032 ----- [+ -]             1.7163\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4552 - Loss Mean: 22.2586 ----- [+ -]         1.8546\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4552 -- Loss: 26.3251 ----- [+ -]             1.6291\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4553 - Loss Mean: 22.4219 ----- [+ -]         1.7948\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4553 -- Loss: 26.5003 ----- [+ -]             1.9068\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4554 - Loss Mean: 22.6021 ----- [+ -]         2.0987\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4554 -- Loss: 26.6731 ----- [+ -]             2.4582\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4555 - Loss Mean: 22.5979 ----- [+ -]         1.8009\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4555 -- Loss: 26.5585 ----- [+ -]             1.3639\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4556 - Loss Mean: 22.4668 ----- [+ -]         1.7399\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4556 -- Loss: 26.3554 ----- [+ -]             2.5240\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4557 - Loss Mean: 22.2929 ----- [+ -]         1.7642\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4557 -- Loss: 26.0601 ----- [+ -]             2.4483\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4558 - Loss Mean: 22.1626 ----- [+ -]         1.8728\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4558 -- Loss: 25.7739 ----- [+ -]             2.5535\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4559 - Loss Mean: 22.1088 ----- [+ -]         2.0049\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4559 -- Loss: 25.8412 ----- [+ -]             2.2905\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4560 - Loss Mean: 22.2167 ----- [+ -]         1.6025\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4560 -- Loss: 25.9008 ----- [+ -]             2.0506\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4561 - Loss Mean: 22.3326 ----- [+ -]         1.8913\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4561 -- Loss: 25.9869 ----- [+ -]             1.6939\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4562 - Loss Mean: 22.4817 ----- [+ -]         2.0800\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4562 -- Loss: 26.3149 ----- [+ -]             2.1769\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4563 - Loss Mean: 22.5478 ----- [+ -]         1.7384\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4563 -- Loss: 26.1514 ----- [+ -]             2.0573\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4564 - Loss Mean: 22.5241 ----- [+ -]         1.7840\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4564 -- Loss: 26.0919 ----- [+ -]             1.4264\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4565 - Loss Mean: 22.3147 ----- [+ -]         1.7468\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4565 -- Loss: 25.8955 ----- [+ -]             1.9940\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4566 - Loss Mean: 22.1343 ----- [+ -]         1.9039\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4566 -- Loss: 25.9589 ----- [+ -]             2.2944\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4567 - Loss Mean: 22.1017 ----- [+ -]         1.5069\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4567 -- Loss: 26.1424 ----- [+ -]             2.1668\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4568 - Loss Mean: 22.1725 ----- [+ -]         1.6955\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4568 -- Loss: 26.2775 ----- [+ -]             0.9883\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4569 - Loss Mean: 22.2915 ----- [+ -]         1.8377\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4569 -- Loss: 26.3423 ----- [+ -]             2.6781\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4570 - Loss Mean: 22.4374 ----- [+ -]         1.9809\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4570 -- Loss: 26.6687 ----- [+ -]             1.8204\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4571 - Loss Mean: 22.5319 ----- [+ -]         1.7402\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4571 -- Loss: 26.4475 ----- [+ -]             1.8670\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4572 - Loss Mean: 22.4537 ----- [+ -]         1.9722\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4572 -- Loss: 26.4831 ----- [+ -]             1.6806\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4573 - Loss Mean: 22.2628 ----- [+ -]         2.0254\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4573 -- Loss: 26.2141 ----- [+ -]             1.7132\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4574 - Loss Mean: 22.0986 ----- [+ -]         2.3199\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4574 -- Loss: 25.8770 ----- [+ -]             2.4560\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4575 - Loss Mean: 22.0429 ----- [+ -]         1.9621\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4575 -- Loss: 25.8512 ----- [+ -]             2.5437\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4576 - Loss Mean: 22.1351 ----- [+ -]         2.0129\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4576 -- Loss: 26.0081 ----- [+ -]             2.0238\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4577 - Loss Mean: 22.3263 ----- [+ -]         2.1042\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4577 -- Loss: 26.0069 ----- [+ -]             3.2556\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4578 - Loss Mean: 22.4372 ----- [+ -]         2.0594\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4578 -- Loss: 25.9443 ----- [+ -]             2.2859\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4579 - Loss Mean: 22.5482 ----- [+ -]         1.6476\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4579 -- Loss: 26.1878 ----- [+ -]             1.8987\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4580 - Loss Mean: 22.4809 ----- [+ -]         1.8124\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4580 -- Loss: 25.9607 ----- [+ -]             2.0700\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4581 - Loss Mean: 22.3072 ----- [+ -]         1.5200\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4581 -- Loss: 25.9611 ----- [+ -]             2.5095\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4582 - Loss Mean: 22.2815 ----- [+ -]         2.2083\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4582 -- Loss: 25.8906 ----- [+ -]             1.9205\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4583 - Loss Mean: 22.0979 ----- [+ -]         1.9529\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4583 -- Loss: 25.9069 ----- [+ -]             1.9310\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4584 - Loss Mean: 22.1161 ----- [+ -]         1.8087\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4584 -- Loss: 26.0942 ----- [+ -]             1.6645\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4585 - Loss Mean: 22.2234 ----- [+ -]         1.9935\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4585 -- Loss: 26.2711 ----- [+ -]             3.6598\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4586 - Loss Mean: 22.4041 ----- [+ -]         1.5839\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4586 -- Loss: 26.3703 ----- [+ -]             2.2603\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4587 - Loss Mean: 22.4176 ----- [+ -]         2.0719\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4587 -- Loss: 26.4221 ----- [+ -]             2.0026\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4588 - Loss Mean: 22.4808 ----- [+ -]         1.8584\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4588 -- Loss: 26.5399 ----- [+ -]             1.6823\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4589 - Loss Mean: 22.3980 ----- [+ -]         1.8349\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4589 -- Loss: 26.1769 ----- [+ -]             2.8307\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4590 - Loss Mean: 22.1186 ----- [+ -]         1.7084\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4590 -- Loss: 26.2553 ----- [+ -]             2.5349\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4591 - Loss Mean: 22.0387 ----- [+ -]         2.1250\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4591 -- Loss: 26.0137 ----- [+ -]             1.7785\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4592 - Loss Mean: 22.1136 ----- [+ -]         1.6648\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4592 -- Loss: 25.9864 ----- [+ -]             2.0493\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4593 - Loss Mean: 22.2059 ----- [+ -]         1.9692\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4593 -- Loss: 25.7606 ----- [+ -]             3.2767\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4594 - Loss Mean: 22.4017 ----- [+ -]         1.9011\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4594 -- Loss: 26.1011 ----- [+ -]             1.5832\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4595 - Loss Mean: 22.4591 ----- [+ -]         1.8957\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4595 -- Loss: 26.0422 ----- [+ -]             1.7738\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4596 - Loss Mean: 22.4624 ----- [+ -]         1.9332\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4596 -- Loss: 25.9791 ----- [+ -]             2.0454\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4597 - Loss Mean: 22.3058 ----- [+ -]         1.7193\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4597 -- Loss: 25.8907 ----- [+ -]             2.1374\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4598 - Loss Mean: 22.1808 ----- [+ -]         2.0760\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4598 -- Loss: 25.9355 ----- [+ -]             1.5297\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4599 - Loss Mean: 22.0497 ----- [+ -]         2.0562\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4599 -- Loss: 26.0222 ----- [+ -]             2.6808\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4600 - Loss Mean: 22.0589 ----- [+ -]         1.8046\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4600 -- Loss: 26.1025 ----- [+ -]             1.6686\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4601 - Loss Mean: 22.1775 ----- [+ -]         1.5351\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4601 -- Loss: 26.3213 ----- [+ -]             1.4438\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4602 - Loss Mean: 22.3531 ----- [+ -]         1.8329\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4602 -- Loss: 26.5592 ----- [+ -]             2.3789\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4603 - Loss Mean: 22.4077 ----- [+ -]         1.8766\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4603 -- Loss: 26.5159 ----- [+ -]             1.6908\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4604 - Loss Mean: 22.3891 ----- [+ -]         1.9237\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4604 -- Loss: 26.3476 ----- [+ -]             2.3634\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4605 - Loss Mean: 22.2840 ----- [+ -]         1.9142\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4605 -- Loss: 26.4243 ----- [+ -]             2.2525\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4606 - Loss Mean: 22.1438 ----- [+ -]         1.8910\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4606 -- Loss: 25.9634 ----- [+ -]             2.3404\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4607 - Loss Mean: 22.1232 ----- [+ -]         2.0520\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4607 -- Loss: 26.0696 ----- [+ -]             1.9383\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4608 - Loss Mean: 22.0859 ----- [+ -]         1.8921\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4608 -- Loss: 25.9065 ----- [+ -]             1.2099\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4609 - Loss Mean: 22.2224 ----- [+ -]         1.8843\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4609 -- Loss: 26.0330 ----- [+ -]             2.0038\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4610 - Loss Mean: 22.2882 ----- [+ -]         1.9790\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4610 -- Loss: 25.9849 ----- [+ -]             1.7973\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4611 - Loss Mean: 22.4234 ----- [+ -]         1.9050\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4611 -- Loss: 26.0895 ----- [+ -]             1.9145\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4612 - Loss Mean: 22.4070 ----- [+ -]         1.7986\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4612 -- Loss: 26.0513 ----- [+ -]             1.8963\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4613 - Loss Mean: 22.2637 ----- [+ -]         1.9509\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4613 -- Loss: 25.9003 ----- [+ -]             1.6270\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4614 - Loss Mean: 22.1038 ----- [+ -]         1.9782\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4614 -- Loss: 25.9313 ----- [+ -]             2.2666\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4615 - Loss Mean: 22.0173 ----- [+ -]         1.8281\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4615 -- Loss: 25.9333 ----- [+ -]             2.5232\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4616 - Loss Mean: 22.0044 ----- [+ -]         1.9629\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4616 -- Loss: 25.9462 ----- [+ -]             2.3886\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4617 - Loss Mean: 22.1162 ----- [+ -]         2.2011\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4617 -- Loss: 26.2964 ----- [+ -]             1.9012\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4618 - Loss Mean: 22.2948 ----- [+ -]         2.0977\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4618 -- Loss: 26.3896 ----- [+ -]             2.0959\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4619 - Loss Mean: 22.4109 ----- [+ -]         2.0604\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4619 -- Loss: 26.5574 ----- [+ -]             1.7551\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4620 - Loss Mean: 22.4059 ----- [+ -]         1.7220\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4620 -- Loss: 26.3822 ----- [+ -]             1.3256\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4621 - Loss Mean: 22.3391 ----- [+ -]         2.3435\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4621 -- Loss: 26.3166 ----- [+ -]             1.3981\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4622 - Loss Mean: 22.0610 ----- [+ -]         2.0888\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4622 -- Loss: 26.0523 ----- [+ -]             1.7620\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4623 - Loss Mean: 22.0111 ----- [+ -]         1.9287\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4623 -- Loss: 26.0115 ----- [+ -]             1.7071\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4624 - Loss Mean: 22.0852 ----- [+ -]         1.7932\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4624 -- Loss: 25.8158 ----- [+ -]             2.8657\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4625 - Loss Mean: 22.1044 ----- [+ -]         2.0360\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4625 -- Loss: 26.0874 ----- [+ -]             2.7377\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4626 - Loss Mean: 22.2787 ----- [+ -]         1.7078\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4626 -- Loss: 26.0155 ----- [+ -]             1.7215\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4627 - Loss Mean: 22.4128 ----- [+ -]         1.8411\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4627 -- Loss: 26.0244 ----- [+ -]             1.9456\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4628 - Loss Mean: 22.3664 ----- [+ -]         1.5843\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4628 -- Loss: 25.9743 ----- [+ -]             1.9260\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4629 - Loss Mean: 22.1822 ----- [+ -]         1.9669\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4629 -- Loss: 26.0870 ----- [+ -]             1.5941\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4630 - Loss Mean: 22.1435 ----- [+ -]         1.8761\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4630 -- Loss: 26.0022 ----- [+ -]             2.0114\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4631 - Loss Mean: 21.9785 ----- [+ -]         1.9340\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4631 -- Loss: 25.8482 ----- [+ -]             2.9292\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4632 - Loss Mean: 22.0272 ----- [+ -]         1.7626\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4632 -- Loss: 26.1734 ----- [+ -]             1.5038\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4633 - Loss Mean: 22.1269 ----- [+ -]         2.2125\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4633 -- Loss: 26.2758 ----- [+ -]             2.2682\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4634 - Loss Mean: 22.3078 ----- [+ -]         2.1833\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4634 -- Loss: 26.4446 ----- [+ -]             1.6191\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4635 - Loss Mean: 22.3385 ----- [+ -]         1.8516\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4635 -- Loss: 26.6064 ----- [+ -]             2.7152\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4636 - Loss Mean: 22.4094 ----- [+ -]         1.7702\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4636 -- Loss: 26.4192 ----- [+ -]             2.2921\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4637 - Loss Mean: 22.2692 ----- [+ -]         1.7781\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4637 -- Loss: 26.2874 ----- [+ -]             1.9440\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4638 - Loss Mean: 22.0732 ----- [+ -]         1.9667\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4638 -- Loss: 26.0082 ----- [+ -]             1.7197\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4639 - Loss Mean: 21.9997 ----- [+ -]         1.8698\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4639 -- Loss: 25.9730 ----- [+ -]             2.2815\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4640 - Loss Mean: 21.9558 ----- [+ -]         1.7223\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4640 -- Loss: 25.8934 ----- [+ -]             1.7848\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4641 - Loss Mean: 22.1418 ----- [+ -]         1.7082\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4641 -- Loss: 25.8015 ----- [+ -]             2.7728\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4642 - Loss Mean: 22.2578 ----- [+ -]         1.7523\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4642 -- Loss: 26.0805 ----- [+ -]             2.5040\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4643 - Loss Mean: 22.4153 ----- [+ -]         1.6596\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4643 -- Loss: 25.9994 ----- [+ -]             1.5134\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4644 - Loss Mean: 22.3616 ----- [+ -]         1.9050\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4644 -- Loss: 25.9479 ----- [+ -]             2.2833\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4645 - Loss Mean: 22.2434 ----- [+ -]         1.7674\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4645 -- Loss: 25.9992 ----- [+ -]             2.0300\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4646 - Loss Mean: 22.1190 ----- [+ -]         1.5056\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4646 -- Loss: 25.9684 ----- [+ -]             2.1952\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4647 - Loss Mean: 21.9969 ----- [+ -]         1.7911\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4647 -- Loss: 26.0192 ----- [+ -]             1.5130\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4648 - Loss Mean: 21.9914 ----- [+ -]         2.1597\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4648 -- Loss: 26.2302 ----- [+ -]             1.9414\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4649 - Loss Mean: 22.1707 ----- [+ -]         1.7733\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4649 -- Loss: 26.2691 ----- [+ -]             2.1325\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4650 - Loss Mean: 22.2655 ----- [+ -]         1.9068\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4650 -- Loss: 26.3623 ----- [+ -]             2.1869\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4651 - Loss Mean: 22.4043 ----- [+ -]         2.0716\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4651 -- Loss: 26.5203 ----- [+ -]             1.2501\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4652 - Loss Mean: 22.3097 ----- [+ -]         1.8788\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4652 -- Loss: 26.4479 ----- [+ -]             3.0968\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4653 - Loss Mean: 22.2602 ----- [+ -]         2.0871\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4653 -- Loss: 26.0942 ----- [+ -]             1.9625\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4654 - Loss Mean: 22.1374 ----- [+ -]         1.8174\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4654 -- Loss: 25.9823 ----- [+ -]             1.7623\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4655 - Loss Mean: 21.9626 ----- [+ -]         1.7999\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4655 -- Loss: 25.8793 ----- [+ -]             1.5198\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4656 - Loss Mean: 22.0229 ----- [+ -]         1.7111\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4656 -- Loss: 26.0338 ----- [+ -]             1.8052\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4657 - Loss Mean: 22.1642 ----- [+ -]         1.8976\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4657 -- Loss: 25.7881 ----- [+ -]             2.1263\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4658 - Loss Mean: 22.3269 ----- [+ -]         1.5332\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4658 -- Loss: 26.0437 ----- [+ -]             2.0449\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4659 - Loss Mean: 22.2966 ----- [+ -]         2.2588\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4659 -- Loss: 25.9729 ----- [+ -]             1.9970\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4660 - Loss Mean: 22.2672 ----- [+ -]         2.0045\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4660 -- Loss: 25.8477 ----- [+ -]             2.6567\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4661 - Loss Mean: 22.1544 ----- [+ -]         1.8224\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4661 -- Loss: 25.9322 ----- [+ -]             2.0557\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4662 - Loss Mean: 22.0515 ----- [+ -]         1.9596\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4662 -- Loss: 25.9037 ----- [+ -]             2.5018\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4663 - Loss Mean: 21.9473 ----- [+ -]         1.7222\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4663 -- Loss: 25.9341 ----- [+ -]             1.7492\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4664 - Loss Mean: 22.0474 ----- [+ -]         1.7802\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4664 -- Loss: 26.2479 ----- [+ -]             2.0919\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4665 - Loss Mean: 22.1774 ----- [+ -]         1.3700\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4665 -- Loss: 26.3572 ----- [+ -]             1.6689\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4666 - Loss Mean: 22.3164 ----- [+ -]         1.4585\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4666 -- Loss: 26.3942 ----- [+ -]             1.9110\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4667 - Loss Mean: 22.3271 ----- [+ -]         1.8351\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4667 -- Loss: 26.4722 ----- [+ -]             1.8369\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4668 - Loss Mean: 22.2869 ----- [+ -]         2.1244\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4668 -- Loss: 26.2408 ----- [+ -]             1.8104\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4669 - Loss Mean: 22.0604 ----- [+ -]         2.0214\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4669 -- Loss: 26.0325 ----- [+ -]             1.5636\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4670 - Loss Mean: 21.9537 ----- [+ -]         2.0242\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4670 -- Loss: 25.8633 ----- [+ -]             1.8047\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4671 - Loss Mean: 21.9101 ----- [+ -]         1.6427\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4671 -- Loss: 25.8489 ----- [+ -]             2.1515\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4672 - Loss Mean: 22.0332 ----- [+ -]         1.8145\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4672 -- Loss: 25.9362 ----- [+ -]             1.9955\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4673 - Loss Mean: 22.1362 ----- [+ -]         1.8215\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4673 -- Loss: 26.0375 ----- [+ -]             2.1892\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4674 - Loss Mean: 22.3221 ----- [+ -]         1.5023\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4674 -- Loss: 26.0250 ----- [+ -]             2.0893\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4675 - Loss Mean: 22.3071 ----- [+ -]         1.8935\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4675 -- Loss: 26.0087 ----- [+ -]             1.7742\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4676 - Loss Mean: 22.2413 ----- [+ -]         1.6453\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4676 -- Loss: 25.9288 ----- [+ -]             2.3672\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4677 - Loss Mean: 22.1085 ----- [+ -]         1.8033\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4677 -- Loss: 25.8633 ----- [+ -]             1.8361\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4678 - Loss Mean: 21.9308 ----- [+ -]         1.7459\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4678 -- Loss: 25.9551 ----- [+ -]             1.5989\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4679 - Loss Mean: 21.9403 ----- [+ -]         1.7565\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4679 -- Loss: 25.9373 ----- [+ -]             1.4145\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4680 - Loss Mean: 22.0259 ----- [+ -]         1.6951\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4680 -- Loss: 26.2861 ----- [+ -]             1.3764\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4681 - Loss Mean: 22.1506 ----- [+ -]         1.9269\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4681 -- Loss: 26.2575 ----- [+ -]             1.9637\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4682 - Loss Mean: 22.3686 ----- [+ -]         2.0645\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4682 -- Loss: 26.4213 ----- [+ -]             2.7941\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4683 - Loss Mean: 22.2664 ----- [+ -]         2.2320\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4683 -- Loss: 26.5260 ----- [+ -]             2.4382\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4684 - Loss Mean: 22.1475 ----- [+ -]         1.9570\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4684 -- Loss: 26.2144 ----- [+ -]             1.8069\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4685 - Loss Mean: 22.0779 ----- [+ -]         1.6657\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4685 -- Loss: 25.9988 ----- [+ -]             2.1388\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4686 - Loss Mean: 21.8962 ----- [+ -]         1.5948\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4686 -- Loss: 25.9469 ----- [+ -]             1.7075\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4687 - Loss Mean: 21.9723 ----- [+ -]         1.6161\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4687 -- Loss: 25.7903 ----- [+ -]             2.0514\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4688 - Loss Mean: 22.0642 ----- [+ -]         1.6735\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4688 -- Loss: 26.0770 ----- [+ -]             2.2965\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4689 - Loss Mean: 22.1900 ----- [+ -]         2.0288\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4689 -- Loss: 26.1508 ----- [+ -]             2.3278\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4690 - Loss Mean: 22.3333 ----- [+ -]         1.8557\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4690 -- Loss: 26.1193 ----- [+ -]             2.1346\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4691 - Loss Mean: 22.2565 ----- [+ -]         1.7686\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4691 -- Loss: 25.9530 ----- [+ -]             1.6345\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4692 - Loss Mean: 22.1912 ----- [+ -]         1.7494\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4692 -- Loss: 25.8277 ----- [+ -]             1.5525\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4693 - Loss Mean: 21.9809 ----- [+ -]         1.6935\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4693 -- Loss: 25.8268 ----- [+ -]             2.4614\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4694 - Loss Mean: 21.9374 ----- [+ -]         1.7457\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4694 -- Loss: 26.0942 ----- [+ -]             2.7686\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4695 - Loss Mean: 21.9829 ----- [+ -]         1.8647\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4695 -- Loss: 26.1289 ----- [+ -]             2.3951\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4696 - Loss Mean: 22.0257 ----- [+ -]         1.8516\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4696 -- Loss: 26.3228 ----- [+ -]             2.0359\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4697 - Loss Mean: 22.1496 ----- [+ -]         1.8033\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4697 -- Loss: 26.3861 ----- [+ -]             1.8032\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4698 - Loss Mean: 22.2513 ----- [+ -]         1.8896\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4698 -- Loss: 26.3801 ----- [+ -]             2.1958\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4699 - Loss Mean: 22.2215 ----- [+ -]         1.9763\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4699 -- Loss: 26.3588 ----- [+ -]             2.0000\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4700 - Loss Mean: 22.0894 ----- [+ -]         1.9893\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4700 -- Loss: 26.1972 ----- [+ -]             2.1277\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4701 - Loss Mean: 21.9511 ----- [+ -]         1.7796\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4701 -- Loss: 26.0227 ----- [+ -]             1.9069\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4702 - Loss Mean: 21.9272 ----- [+ -]         1.8208\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4702 -- Loss: 25.9943 ----- [+ -]             2.4425\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4703 - Loss Mean: 21.9137 ----- [+ -]         1.7962\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4703 -- Loss: 25.9442 ----- [+ -]             1.6604\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4704 - Loss Mean: 22.0867 ----- [+ -]         2.3012\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4704 -- Loss: 25.9674 ----- [+ -]             1.9862\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4705 - Loss Mean: 22.3195 ----- [+ -]         2.0944\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4705 -- Loss: 26.0423 ----- [+ -]             1.7682\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4706 - Loss Mean: 22.3501 ----- [+ -]         1.9483\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4706 -- Loss: 26.2360 ----- [+ -]             3.1059\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4707 - Loss Mean: 22.2682 ----- [+ -]         1.5203\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4707 -- Loss: 25.8005 ----- [+ -]             2.8329\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4708 - Loss Mean: 22.0836 ----- [+ -]         1.4386\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4708 -- Loss: 25.8703 ----- [+ -]             2.6589\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4709 - Loss Mean: 22.0672 ----- [+ -]         2.0918\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4709 -- Loss: 25.9451 ----- [+ -]             1.3805\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4710 - Loss Mean: 21.8699 ----- [+ -]         1.8229\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4710 -- Loss: 25.9993 ----- [+ -]             1.4503\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4711 - Loss Mean: 21.8792 ----- [+ -]         1.7577\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4711 -- Loss: 26.2116 ----- [+ -]             2.8485\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4712 - Loss Mean: 22.0193 ----- [+ -]         1.6987\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4712 -- Loss: 26.2859 ----- [+ -]             2.0530\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4713 - Loss Mean: 22.2933 ----- [+ -]         1.7581\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4713 -- Loss: 26.4822 ----- [+ -]             1.2871\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4714 - Loss Mean: 22.2607 ----- [+ -]         1.9674\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4714 -- Loss: 26.3274 ----- [+ -]             1.9665\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4715 - Loss Mean: 22.2045 ----- [+ -]         1.7534\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4715 -- Loss: 26.3498 ----- [+ -]             1.5508\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4716 - Loss Mean: 22.0134 ----- [+ -]         1.7446\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4716 -- Loss: 26.0773 ----- [+ -]             2.3553\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4717 - Loss Mean: 21.9346 ----- [+ -]         1.8099\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4717 -- Loss: 25.8812 ----- [+ -]             2.5760\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4718 - Loss Mean: 21.8471 ----- [+ -]         2.0473\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4718 -- Loss: 26.0438 ----- [+ -]             2.4205\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4719 - Loss Mean: 21.9250 ----- [+ -]         1.7609\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4719 -- Loss: 26.0761 ----- [+ -]             2.2261\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4720 - Loss Mean: 22.1480 ----- [+ -]         1.9748\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4720 -- Loss: 25.9391 ----- [+ -]             2.0991\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4721 - Loss Mean: 22.2168 ----- [+ -]         1.9510\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4721 -- Loss: 26.0026 ----- [+ -]             1.8964\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4722 - Loss Mean: 22.2518 ----- [+ -]         1.6823\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4722 -- Loss: 26.0334 ----- [+ -]             2.1052\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4723 - Loss Mean: 22.1753 ----- [+ -]         1.6760\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4723 -- Loss: 25.7748 ----- [+ -]             2.5027\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4724 - Loss Mean: 22.0514 ----- [+ -]         1.6151\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4724 -- Loss: 25.8449 ----- [+ -]             1.5669\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4725 - Loss Mean: 21.9276 ----- [+ -]         1.8764\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4725 -- Loss: 25.8315 ----- [+ -]             1.9916\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4726 - Loss Mean: 21.8782 ----- [+ -]         1.7458\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4726 -- Loss: 26.0072 ----- [+ -]             2.2860\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4727 - Loss Mean: 21.8931 ----- [+ -]         1.9201\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4727 -- Loss: 26.2173 ----- [+ -]             2.4635\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4728 - Loss Mean: 22.1707 ----- [+ -]         1.9225\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4728 -- Loss: 26.2857 ----- [+ -]             2.5968\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4729 - Loss Mean: 22.2213 ----- [+ -]         1.8278\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4729 -- Loss: 26.4945 ----- [+ -]             1.7804\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4730 - Loss Mean: 22.2655 ----- [+ -]         2.0454\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4730 -- Loss: 26.4044 ----- [+ -]             2.8797\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4731 - Loss Mean: 22.0788 ----- [+ -]         2.0829\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4731 -- Loss: 26.2691 ----- [+ -]             1.9396\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4732 - Loss Mean: 21.9704 ----- [+ -]         1.8239\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4732 -- Loss: 26.1175 ----- [+ -]             1.8881\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4733 - Loss Mean: 21.7877 ----- [+ -]         2.2015\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4733 -- Loss: 25.8885 ----- [+ -]             1.8556\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4734 - Loss Mean: 21.8495 ----- [+ -]         1.5994\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4734 -- Loss: 25.9664 ----- [+ -]             1.7295\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4735 - Loss Mean: 22.0450 ----- [+ -]         1.7006\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4735 -- Loss: 25.7554 ----- [+ -]             2.6986\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4736 - Loss Mean: 22.1003 ----- [+ -]         2.0887\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4736 -- Loss: 26.0660 ----- [+ -]             1.9448\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4737 - Loss Mean: 22.2128 ----- [+ -]         1.8856\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4737 -- Loss: 26.1703 ----- [+ -]             2.0241\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4738 - Loss Mean: 22.2006 ----- [+ -]         2.0882\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4738 -- Loss: 25.8570 ----- [+ -]             2.4714\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4739 - Loss Mean: 22.0653 ----- [+ -]         1.8555\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4739 -- Loss: 26.0241 ----- [+ -]             1.8293\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4740 - Loss Mean: 22.0052 ----- [+ -]         1.9568\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4740 -- Loss: 25.8561 ----- [+ -]             1.3613\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4741 - Loss Mean: 21.8763 ----- [+ -]         1.8122\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4741 -- Loss: 25.8546 ----- [+ -]             2.2666\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4742 - Loss Mean: 21.9285 ----- [+ -]         1.8330\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4742 -- Loss: 25.9254 ----- [+ -]             3.0435\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4743 - Loss Mean: 22.0613 ----- [+ -]         1.9436\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4743 -- Loss: 26.3730 ----- [+ -]             2.7291\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4744 - Loss Mean: 22.0934 ----- [+ -]         2.2674\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4744 -- Loss: 26.4037 ----- [+ -]             2.2674\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4745 - Loss Mean: 22.1847 ----- [+ -]         2.1872\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4745 -- Loss: 26.4318 ----- [+ -]             2.3849\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4746 - Loss Mean: 22.2100 ----- [+ -]         2.0081\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4746 -- Loss: 26.2229 ----- [+ -]             2.2918\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4747 - Loss Mean: 22.0382 ----- [+ -]         1.9100\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4747 -- Loss: 26.2629 ----- [+ -]             2.4084\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4748 - Loss Mean: 21.8540 ----- [+ -]         1.8285\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4748 -- Loss: 26.0457 ----- [+ -]             1.3850\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4749 - Loss Mean: 21.8234 ----- [+ -]         1.5109\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4749 -- Loss: 25.8948 ----- [+ -]             1.4066\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4750 - Loss Mean: 21.8588 ----- [+ -]         1.9892\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4750 -- Loss: 25.9580 ----- [+ -]             1.4805\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4751 - Loss Mean: 21.9648 ----- [+ -]         1.5745\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4751 -- Loss: 26.0394 ----- [+ -]             1.8167\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4752 - Loss Mean: 22.1461 ----- [+ -]         2.0052\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4752 -- Loss: 25.9711 ----- [+ -]             2.2736\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4753 - Loss Mean: 22.1792 ----- [+ -]         1.9431\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4753 -- Loss: 26.0903 ----- [+ -]             2.8568\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4754 - Loss Mean: 22.0999 ----- [+ -]         1.8290\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4754 -- Loss: 25.9579 ----- [+ -]             1.5533\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4755 - Loss Mean: 22.1008 ----- [+ -]         1.7381\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4755 -- Loss: 25.7832 ----- [+ -]             2.0400\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4756 - Loss Mean: 21.8642 ----- [+ -]         2.0534\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4756 -- Loss: 25.8220 ----- [+ -]             2.3643\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4757 - Loss Mean: 21.8126 ----- [+ -]         1.6771\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4757 -- Loss: 25.8247 ----- [+ -]             2.1764\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4758 - Loss Mean: 21.8559 ----- [+ -]         1.8462\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4758 -- Loss: 26.2212 ----- [+ -]             1.8690\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4759 - Loss Mean: 22.0543 ----- [+ -]         1.8467\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4759 -- Loss: 26.2907 ----- [+ -]             2.0349\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4760 - Loss Mean: 22.2138 ----- [+ -]         1.7874\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4760 -- Loss: 26.6337 ----- [+ -]             2.4500\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4761 - Loss Mean: 22.2300 ----- [+ -]         1.6493\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4761 -- Loss: 26.3994 ----- [+ -]             2.0194\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4762 - Loss Mean: 22.1197 ----- [+ -]         1.7512\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4762 -- Loss: 26.2923 ----- [+ -]             2.2330\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4763 - Loss Mean: 21.9420 ----- [+ -]         2.0863\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4763 -- Loss: 26.1463 ----- [+ -]             2.3689\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4764 - Loss Mean: 21.8142 ----- [+ -]         1.7944\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4764 -- Loss: 25.8421 ----- [+ -]             1.7794\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4765 - Loss Mean: 21.8693 ----- [+ -]         1.9352\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4765 -- Loss: 25.8101 ----- [+ -]             2.4388\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4766 - Loss Mean: 22.0200 ----- [+ -]         2.1652\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4766 -- Loss: 26.0036 ----- [+ -]             1.9581\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4767 - Loss Mean: 22.0615 ----- [+ -]         1.8180\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4767 -- Loss: 26.0976 ----- [+ -]             2.5923\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4768 - Loss Mean: 22.1310 ----- [+ -]         1.9704\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4768 -- Loss: 26.0898 ----- [+ -]             1.3218\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4769 - Loss Mean: 22.2189 ----- [+ -]         1.7060\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4769 -- Loss: 25.9717 ----- [+ -]             1.8590\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4770 - Loss Mean: 22.1365 ----- [+ -]         2.0117\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4770 -- Loss: 26.0110 ----- [+ -]             2.1828\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4771 - Loss Mean: 21.9135 ----- [+ -]         1.9009\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4771 -- Loss: 25.8608 ----- [+ -]             1.4407\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4772 - Loss Mean: 21.8089 ----- [+ -]         1.4898\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4772 -- Loss: 25.8382 ----- [+ -]             2.9073\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4773 - Loss Mean: 21.8773 ----- [+ -]         1.9405\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4773 -- Loss: 26.0569 ----- [+ -]             1.8919\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4774 - Loss Mean: 21.9521 ----- [+ -]         1.8370\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4774 -- Loss: 26.1743 ----- [+ -]             1.9375\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4775 - Loss Mean: 22.1380 ----- [+ -]         1.8697\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4775 -- Loss: 26.4725 ----- [+ -]             1.6411\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4776 - Loss Mean: 22.2817 ----- [+ -]         2.3339\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4776 -- Loss: 26.5177 ----- [+ -]             1.8158\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4777 - Loss Mean: 22.1602 ----- [+ -]         1.8032\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4777 -- Loss: 26.3165 ----- [+ -]             2.3404\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4778 - Loss Mean: 22.0476 ----- [+ -]         1.8094\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4778 -- Loss: 26.2407 ----- [+ -]             2.4607\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4779 - Loss Mean: 21.9437 ----- [+ -]         1.9258\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4779 -- Loss: 26.2169 ----- [+ -]             2.3012\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4780 - Loss Mean: 21.8085 ----- [+ -]         2.0230\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4780 -- Loss: 25.9324 ----- [+ -]             2.4006\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4781 - Loss Mean: 21.7738 ----- [+ -]         1.9984\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4781 -- Loss: 26.0249 ----- [+ -]             1.8860\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4782 - Loss Mean: 22.0022 ----- [+ -]         1.5148\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4782 -- Loss: 25.8242 ----- [+ -]             2.5043\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4783 - Loss Mean: 22.0887 ----- [+ -]         1.8038\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4783 -- Loss: 25.9828 ----- [+ -]             2.9407\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4784 - Loss Mean: 22.1426 ----- [+ -]         1.7451\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4784 -- Loss: 25.8988 ----- [+ -]             2.0687\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4785 - Loss Mean: 22.0980 ----- [+ -]         2.0035\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4785 -- Loss: 25.8922 ----- [+ -]             2.2411\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4786 - Loss Mean: 21.9990 ----- [+ -]         2.1387\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4786 -- Loss: 25.8812 ----- [+ -]             2.4861\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4787 - Loss Mean: 21.8604 ----- [+ -]         1.7691\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4787 -- Loss: 26.0504 ----- [+ -]             1.9778\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4788 - Loss Mean: 21.8586 ----- [+ -]         1.7436\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4788 -- Loss: 25.8806 ----- [+ -]             2.1866\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4789 - Loss Mean: 21.8787 ----- [+ -]         1.7352\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4789 -- Loss: 26.1794 ----- [+ -]             1.5714\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4790 - Loss Mean: 21.9585 ----- [+ -]         2.1829\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4790 -- Loss: 26.2178 ----- [+ -]             2.5628\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4791 - Loss Mean: 22.2394 ----- [+ -]         1.8757\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4791 -- Loss: 26.5328 ----- [+ -]             2.9287\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4792 - Loss Mean: 22.1974 ----- [+ -]         2.2558\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4792 -- Loss: 26.4946 ----- [+ -]             1.8692\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4793 - Loss Mean: 22.1595 ----- [+ -]         1.7473\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4793 -- Loss: 26.3980 ----- [+ -]             3.3746\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4794 - Loss Mean: 21.9748 ----- [+ -]         1.8876\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4794 -- Loss: 26.1191 ----- [+ -]             1.7474\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4795 - Loss Mean: 21.7496 ----- [+ -]         2.0981\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4795 -- Loss: 25.9489 ----- [+ -]             2.0183\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4796 - Loss Mean: 21.8005 ----- [+ -]         1.8839\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4796 -- Loss: 25.9105 ----- [+ -]             1.6962\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4797 - Loss Mean: 21.9093 ----- [+ -]         1.6563\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4797 -- Loss: 25.8606 ----- [+ -]             2.3492\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4798 - Loss Mean: 22.0614 ----- [+ -]         1.4868\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4798 -- Loss: 25.9977 ----- [+ -]             1.7669\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4799 - Loss Mean: 22.1229 ----- [+ -]         2.1611\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4799 -- Loss: 26.0249 ----- [+ -]             2.8215\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4800 - Loss Mean: 22.2646 ----- [+ -]         1.9763\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4800 -- Loss: 26.0149 ----- [+ -]             1.9063\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4801 - Loss Mean: 22.0259 ----- [+ -]         2.0817\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4801 -- Loss: 25.8716 ----- [+ -]             1.9975\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4802 - Loss Mean: 21.8760 ----- [+ -]         1.8824\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4802 -- Loss: 25.8330 ----- [+ -]             2.7803\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4803 - Loss Mean: 21.8070 ----- [+ -]         2.2000\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4803 -- Loss: 25.9413 ----- [+ -]             1.6113\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4804 - Loss Mean: 21.7914 ----- [+ -]         1.4995\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4804 -- Loss: 26.1692 ----- [+ -]             1.9115\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4805 - Loss Mean: 21.8854 ----- [+ -]         1.6722\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4805 -- Loss: 26.3159 ----- [+ -]             2.0416\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4806 - Loss Mean: 22.1420 ----- [+ -]         1.9612\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4806 -- Loss: 26.4548 ----- [+ -]             2.2988\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4807 - Loss Mean: 22.1629 ----- [+ -]         1.5354\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4807 -- Loss: 26.3730 ----- [+ -]             1.9198\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4808 - Loss Mean: 22.1094 ----- [+ -]         2.1154\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4808 -- Loss: 26.4128 ----- [+ -]             2.4156\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4809 - Loss Mean: 21.9807 ----- [+ -]         1.5762\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4809 -- Loss: 26.0821 ----- [+ -]             2.5415\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4810 - Loss Mean: 21.8702 ----- [+ -]         1.7297\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4810 -- Loss: 26.1003 ----- [+ -]             2.2449\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4811 - Loss Mean: 21.8066 ----- [+ -]         1.8041\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4811 -- Loss: 25.9085 ----- [+ -]             1.4406\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4812 - Loss Mean: 21.8614 ----- [+ -]         1.8045\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4812 -- Loss: 25.8487 ----- [+ -]             1.3226\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4813 - Loss Mean: 21.9628 ----- [+ -]         1.7720\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4813 -- Loss: 25.9301 ----- [+ -]             2.6270\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4814 - Loss Mean: 22.0883 ----- [+ -]         2.3347\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4814 -- Loss: 25.9925 ----- [+ -]             1.8516\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4815 - Loss Mean: 22.2451 ----- [+ -]         2.0113\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4815 -- Loss: 25.9936 ----- [+ -]             1.5620\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4816 - Loss Mean: 22.1071 ----- [+ -]         1.6443\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4816 -- Loss: 25.9281 ----- [+ -]             1.9138\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4817 - Loss Mean: 21.9314 ----- [+ -]         1.6742\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4817 -- Loss: 25.9225 ----- [+ -]             2.7650\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4818 - Loss Mean: 21.8014 ----- [+ -]         2.1292\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4818 -- Loss: 26.0333 ----- [+ -]             1.9547\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4819 - Loss Mean: 21.7600 ----- [+ -]         2.0870\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4819 -- Loss: 26.1855 ----- [+ -]             2.4818\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4820 - Loss Mean: 21.8526 ----- [+ -]         2.1927\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4820 -- Loss: 26.1674 ----- [+ -]             2.0837\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4821 - Loss Mean: 21.9987 ----- [+ -]         1.8777\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4821 -- Loss: 26.2949 ----- [+ -]             1.9774\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4822 - Loss Mean: 22.1171 ----- [+ -]         1.9944\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4822 -- Loss: 26.3115 ----- [+ -]             2.2759\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4823 - Loss Mean: 22.1015 ----- [+ -]         1.7718\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4823 -- Loss: 26.4740 ----- [+ -]             1.7547\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4824 - Loss Mean: 22.0592 ----- [+ -]         2.1223\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4824 -- Loss: 26.2039 ----- [+ -]             1.6377\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4825 - Loss Mean: 21.8269 ----- [+ -]         1.9319\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4825 -- Loss: 26.0581 ----- [+ -]             1.9686\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4826 - Loss Mean: 21.7140 ----- [+ -]         1.8500\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4826 -- Loss: 25.9052 ----- [+ -]             2.5546\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4827 - Loss Mean: 21.7703 ----- [+ -]         1.6782\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4827 -- Loss: 25.9266 ----- [+ -]             2.2241\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4828 - Loss Mean: 21.9846 ----- [+ -]         1.6367\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4828 -- Loss: 26.0859 ----- [+ -]             1.8396\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4829 - Loss Mean: 22.0964 ----- [+ -]         1.6622\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4829 -- Loss: 26.1641 ----- [+ -]             1.7419\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4830 - Loss Mean: 22.1570 ----- [+ -]         1.8332\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4830 -- Loss: 26.0683 ----- [+ -]             2.2246\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4831 - Loss Mean: 22.0900 ----- [+ -]         1.6497\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4831 -- Loss: 25.9805 ----- [+ -]             1.9116\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4832 - Loss Mean: 21.9473 ----- [+ -]         1.6596\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4832 -- Loss: 26.1005 ----- [+ -]             2.8774\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4833 - Loss Mean: 21.7836 ----- [+ -]         1.8122\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4833 -- Loss: 26.0065 ----- [+ -]             2.1547\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4834 - Loss Mean: 21.7265 ----- [+ -]         2.2771\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4834 -- Loss: 26.0495 ----- [+ -]             1.9400\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4835 - Loss Mean: 21.9090 ----- [+ -]         1.9438\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4835 -- Loss: 26.1418 ----- [+ -]             2.3557\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4836 - Loss Mean: 21.9626 ----- [+ -]         2.1034\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4836 -- Loss: 26.3049 ----- [+ -]             1.8388\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4837 - Loss Mean: 22.0773 ----- [+ -]         2.1855\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4837 -- Loss: 26.4573 ----- [+ -]             2.8362\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4838 - Loss Mean: 22.1014 ----- [+ -]         2.3131\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4838 -- Loss: 26.4115 ----- [+ -]             1.7081\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4839 - Loss Mean: 22.0589 ----- [+ -]         2.0117\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4839 -- Loss: 26.3164 ----- [+ -]             2.3811\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4840 - Loss Mean: 21.9797 ----- [+ -]         2.0563\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4840 -- Loss: 26.1063 ----- [+ -]             1.7228\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4841 - Loss Mean: 21.7406 ----- [+ -]         1.8721\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4841 -- Loss: 26.1526 ----- [+ -]             2.4971\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4842 - Loss Mean: 21.7861 ----- [+ -]         1.9828\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4842 -- Loss: 25.8941 ----- [+ -]             2.5688\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4843 - Loss Mean: 21.8851 ----- [+ -]         2.0490\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4843 -- Loss: 25.9014 ----- [+ -]             1.9271\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4844 - Loss Mean: 22.1866 ----- [+ -]         2.3015\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4844 -- Loss: 25.9666 ----- [+ -]             1.8051\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4845 - Loss Mean: 22.1253 ----- [+ -]         1.8194\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4845 -- Loss: 25.9704 ----- [+ -]             1.9403\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4846 - Loss Mean: 22.0528 ----- [+ -]         1.9536\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4846 -- Loss: 26.1825 ----- [+ -]             2.0634\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4847 - Loss Mean: 21.9773 ----- [+ -]         1.9074\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4847 -- Loss: 25.9150 ----- [+ -]             2.6469\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4848 - Loss Mean: 21.8644 ----- [+ -]         1.6644\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4848 -- Loss: 26.1213 ----- [+ -]             3.1921\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4849 - Loss Mean: 21.7942 ----- [+ -]         1.9856\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4849 -- Loss: 26.0065 ----- [+ -]             2.4022\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4850 - Loss Mean: 21.7909 ----- [+ -]         1.8624\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4850 -- Loss: 26.0831 ----- [+ -]             2.2667\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4851 - Loss Mean: 21.9853 ----- [+ -]         1.5930\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4851 -- Loss: 26.3783 ----- [+ -]             1.7315\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4852 - Loss Mean: 22.1806 ----- [+ -]         1.9981\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4852 -- Loss: 26.4511 ----- [+ -]             2.1163\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4853 - Loss Mean: 22.2662 ----- [+ -]         2.0667\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4853 -- Loss: 26.3732 ----- [+ -]             2.3110\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4854 - Loss Mean: 22.1043 ----- [+ -]         1.9690\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4854 -- Loss: 26.3311 ----- [+ -]             1.9117\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4855 - Loss Mean: 21.8792 ----- [+ -]         1.9074\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4855 -- Loss: 26.1929 ----- [+ -]             2.7996\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4856 - Loss Mean: 21.7180 ----- [+ -]         1.7613\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4856 -- Loss: 25.9969 ----- [+ -]             2.4596\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4857 - Loss Mean: 21.8074 ----- [+ -]         1.6922\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4857 -- Loss: 25.8460 ----- [+ -]             1.6954\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4858 - Loss Mean: 21.9100 ----- [+ -]         1.7159\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4858 -- Loss: 25.9586 ----- [+ -]             1.7314\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4859 - Loss Mean: 22.0488 ----- [+ -]         1.7316\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4859 -- Loss: 25.9857 ----- [+ -]             1.6756\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4860 - Loss Mean: 22.1681 ----- [+ -]         1.9416\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4860 -- Loss: 26.0215 ----- [+ -]             1.2587\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4861 - Loss Mean: 22.2402 ----- [+ -]         2.0201\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4861 -- Loss: 26.0522 ----- [+ -]             1.6053\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4862 - Loss Mean: 21.9840 ----- [+ -]         2.0470\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4862 -- Loss: 25.8669 ----- [+ -]             2.4801\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4863 - Loss Mean: 21.7621 ----- [+ -]         1.9660\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4863 -- Loss: 26.0901 ----- [+ -]             2.0676\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4864 - Loss Mean: 21.7469 ----- [+ -]         1.6834\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4864 -- Loss: 25.9741 ----- [+ -]             1.8053\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4865 - Loss Mean: 21.7667 ----- [+ -]         1.9783\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4865 -- Loss: 26.1530 ----- [+ -]             2.1021\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4866 - Loss Mean: 22.0096 ----- [+ -]         1.9214\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4866 -- Loss: 26.2957 ----- [+ -]             1.7763\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4867 - Loss Mean: 22.0822 ----- [+ -]         1.6733\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4867 -- Loss: 26.5184 ----- [+ -]             1.5548\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4868 - Loss Mean: 22.1629 ----- [+ -]         1.7812\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4868 -- Loss: 26.4682 ----- [+ -]             2.1424\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4869 - Loss Mean: 22.0702 ----- [+ -]         2.3795\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4869 -- Loss: 26.2998 ----- [+ -]             2.4082\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4870 - Loss Mean: 21.9986 ----- [+ -]         2.1813\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4870 -- Loss: 26.2830 ----- [+ -]             2.5694\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4871 - Loss Mean: 21.7022 ----- [+ -]         1.9599\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4871 -- Loss: 25.9869 ----- [+ -]             2.0604\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4872 - Loss Mean: 21.7358 ----- [+ -]         1.8100\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4872 -- Loss: 25.9246 ----- [+ -]             2.0594\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4873 - Loss Mean: 21.8370 ----- [+ -]         2.2330\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4873 -- Loss: 25.8196 ----- [+ -]             1.8403\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4874 - Loss Mean: 22.0168 ----- [+ -]         1.6569\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4874 -- Loss: 26.0144 ----- [+ -]             2.6248\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4875 - Loss Mean: 22.1211 ----- [+ -]         1.9848\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4875 -- Loss: 25.9249 ----- [+ -]             2.4348\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4876 - Loss Mean: 22.1198 ----- [+ -]         1.6287\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4876 -- Loss: 25.9299 ----- [+ -]             1.8477\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4877 - Loss Mean: 22.0411 ----- [+ -]         1.6960\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4877 -- Loss: 25.8989 ----- [+ -]             1.5183\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4878 - Loss Mean: 21.8027 ----- [+ -]         2.0385\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4878 -- Loss: 25.9674 ----- [+ -]             1.6085\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4879 - Loss Mean: 21.7505 ----- [+ -]         1.7669\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4879 -- Loss: 26.0963 ----- [+ -]             2.4223\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4880 - Loss Mean: 21.7203 ----- [+ -]         1.8578\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4880 -- Loss: 26.1110 ----- [+ -]             1.2807\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4881 - Loss Mean: 21.9326 ----- [+ -]         2.0443\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4881 -- Loss: 26.2839 ----- [+ -]             2.2962\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4882 - Loss Mean: 22.1656 ----- [+ -]         1.9843\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4882 -- Loss: 26.6298 ----- [+ -]             1.5867\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4883 - Loss Mean: 22.1968 ----- [+ -]         1.6009\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4883 -- Loss: 26.5382 ----- [+ -]             1.8617\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4884 - Loss Mean: 22.1371 ----- [+ -]         1.7107\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4884 -- Loss: 26.4702 ----- [+ -]             2.1821\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4885 - Loss Mean: 21.9693 ----- [+ -]         1.5685\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4885 -- Loss: 26.3629 ----- [+ -]             2.0317\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4886 - Loss Mean: 21.8271 ----- [+ -]         1.9942\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4886 -- Loss: 26.0126 ----- [+ -]             1.4746\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4887 - Loss Mean: 21.7328 ----- [+ -]         1.7282\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4887 -- Loss: 26.1102 ----- [+ -]             2.3648\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4888 - Loss Mean: 21.7951 ----- [+ -]         1.8316\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4888 -- Loss: 26.1177 ----- [+ -]             2.5745\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4889 - Loss Mean: 21.9874 ----- [+ -]         1.6320\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4889 -- Loss: 26.2031 ----- [+ -]             1.8674\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4890 - Loss Mean: 22.1730 ----- [+ -]         2.0028\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4890 -- Loss: 26.1178 ----- [+ -]             1.3276\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4891 - Loss Mean: 22.2700 ----- [+ -]         2.2568\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4891 -- Loss: 26.1426 ----- [+ -]             2.0654\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4892 - Loss Mean: 22.0877 ----- [+ -]         1.6808\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4892 -- Loss: 26.0342 ----- [+ -]             1.7606\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4893 - Loss Mean: 21.9177 ----- [+ -]         1.9806\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4893 -- Loss: 25.9146 ----- [+ -]             1.9676\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4894 - Loss Mean: 21.6508 ----- [+ -]         1.9709\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4894 -- Loss: 26.1557 ----- [+ -]             2.0889\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4895 - Loss Mean: 21.6880 ----- [+ -]         1.6315\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4895 -- Loss: 26.0790 ----- [+ -]             2.4268\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4896 - Loss Mean: 21.9455 ----- [+ -]         2.4085\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4896 -- Loss: 26.5428 ----- [+ -]             2.7090\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4897 - Loss Mean: 22.0276 ----- [+ -]         2.1608\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4897 -- Loss: 26.4223 ----- [+ -]             2.1364\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4898 - Loss Mean: 22.2278 ----- [+ -]         1.8459\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4898 -- Loss: 26.7550 ----- [+ -]             2.4827\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4899 - Loss Mean: 22.2645 ----- [+ -]         1.9778\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4899 -- Loss: 26.6577 ----- [+ -]             2.4135\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4900 - Loss Mean: 21.9688 ----- [+ -]         1.8707\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4900 -- Loss: 26.3420 ----- [+ -]             2.7067\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4901 - Loss Mean: 21.8157 ----- [+ -]         2.0137\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4901 -- Loss: 25.9107 ----- [+ -]             2.2469\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4902 - Loss Mean: 21.6855 ----- [+ -]         1.7561\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4902 -- Loss: 26.0085 ----- [+ -]             1.6662\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4903 - Loss Mean: 21.7578 ----- [+ -]         1.8535\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4903 -- Loss: 26.1380 ----- [+ -]             2.5556\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4904 - Loss Mean: 21.9881 ----- [+ -]         1.8132\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4904 -- Loss: 25.9585 ----- [+ -]             1.6400\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4905 - Loss Mean: 22.2235 ----- [+ -]         1.7203\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4905 -- Loss: 26.0819 ----- [+ -]             2.1468\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4906 - Loss Mean: 22.1850 ----- [+ -]         1.9608\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4906 -- Loss: 26.1523 ----- [+ -]             1.4486\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4907 - Loss Mean: 22.1317 ----- [+ -]         1.7624\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4907 -- Loss: 26.0272 ----- [+ -]             1.7069\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4908 - Loss Mean: 21.9059 ----- [+ -]         1.8813\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4908 -- Loss: 26.0145 ----- [+ -]             2.0884\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4909 - Loss Mean: 21.7547 ----- [+ -]         1.7678\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4909 -- Loss: 26.0759 ----- [+ -]             2.1321\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4910 - Loss Mean: 21.6596 ----- [+ -]         1.9797\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4910 -- Loss: 26.0340 ----- [+ -]             1.9944\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4911 - Loss Mean: 21.7722 ----- [+ -]         1.8614\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4911 -- Loss: 26.3940 ----- [+ -]             1.9273\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4912 - Loss Mean: 21.9263 ----- [+ -]         2.1605\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4912 -- Loss: 26.4789 ----- [+ -]             2.3671\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4913 - Loss Mean: 22.2092 ----- [+ -]         1.8120\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4913 -- Loss: 26.7239 ----- [+ -]             2.3937\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4914 - Loss Mean: 22.3523 ----- [+ -]         2.2803\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4914 -- Loss: 26.5106 ----- [+ -]             2.6069\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4915 - Loss Mean: 22.1338 ----- [+ -]         1.6216\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4915 -- Loss: 26.4528 ----- [+ -]             1.6747\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4916 - Loss Mean: 21.9162 ----- [+ -]         2.0100\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4916 -- Loss: 26.0889 ----- [+ -]             1.7458\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4917 - Loss Mean: 21.6972 ----- [+ -]         1.5748\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4917 -- Loss: 25.9841 ----- [+ -]             1.1957\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4918 - Loss Mean: 21.7119 ----- [+ -]         1.9635\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4918 -- Loss: 25.9007 ----- [+ -]             2.6757\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4919 - Loss Mean: 21.8356 ----- [+ -]         1.9968\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4919 -- Loss: 25.9439 ----- [+ -]             2.5834\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4920 - Loss Mean: 22.1219 ----- [+ -]         2.0081\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4920 -- Loss: 26.1069 ----- [+ -]             1.9227\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4921 - Loss Mean: 22.2216 ----- [+ -]         1.5761\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4921 -- Loss: 26.1434 ----- [+ -]             2.1130\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4922 - Loss Mean: 22.1866 ----- [+ -]         1.7475\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4922 -- Loss: 25.9857 ----- [+ -]             2.2540\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4923 - Loss Mean: 22.0656 ----- [+ -]         1.7272\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4923 -- Loss: 26.0341 ----- [+ -]             1.7898\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4924 - Loss Mean: 21.8411 ----- [+ -]         1.7870\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4924 -- Loss: 26.1382 ----- [+ -]             2.3651\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4925 - Loss Mean: 21.7450 ----- [+ -]         1.9264\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4925 -- Loss: 26.0554 ----- [+ -]             1.6093\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4926 - Loss Mean: 21.6437 ----- [+ -]         1.8039\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4926 -- Loss: 26.2778 ----- [+ -]             1.9004\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4927 - Loss Mean: 21.9261 ----- [+ -]         2.1189\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4927 -- Loss: 26.3122 ----- [+ -]             1.6610\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4928 - Loss Mean: 22.0864 ----- [+ -]         1.5685\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4928 -- Loss: 26.3829 ----- [+ -]             2.5648\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4929 - Loss Mean: 22.2269 ----- [+ -]         1.6454\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4929 -- Loss: 26.5879 ----- [+ -]             2.0261\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4930 - Loss Mean: 22.1674 ----- [+ -]         1.9461\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4930 -- Loss: 26.5516 ----- [+ -]             2.3839\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4931 - Loss Mean: 21.9656 ----- [+ -]         1.9635\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4931 -- Loss: 26.4817 ----- [+ -]             2.4691\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4932 - Loss Mean: 21.8412 ----- [+ -]         1.7273\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4932 -- Loss: 26.1352 ----- [+ -]             2.6796\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4933 - Loss Mean: 21.6846 ----- [+ -]         1.9519\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4933 -- Loss: 25.9298 ----- [+ -]             1.6061\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4934 - Loss Mean: 21.7878 ----- [+ -]         1.8658\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4934 -- Loss: 25.9467 ----- [+ -]             2.1469\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4935 - Loss Mean: 21.9183 ----- [+ -]         1.6942\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4935 -- Loss: 26.0152 ----- [+ -]             1.3229\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4936 - Loss Mean: 22.1904 ----- [+ -]         1.9819\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4936 -- Loss: 26.1422 ----- [+ -]             2.0688\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4937 - Loss Mean: 22.2674 ----- [+ -]         1.6104\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4937 -- Loss: 26.0829 ----- [+ -]             2.4045\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4938 - Loss Mean: 22.2019 ----- [+ -]         1.8813\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4938 -- Loss: 25.9970 ----- [+ -]             1.9688\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4939 - Loss Mean: 22.0385 ----- [+ -]         1.7537\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4939 -- Loss: 25.8986 ----- [+ -]             1.4402\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4940 - Loss Mean: 21.7749 ----- [+ -]         1.8744\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4940 -- Loss: 25.8256 ----- [+ -]             2.5426\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4941 - Loss Mean: 21.6729 ----- [+ -]         1.9390\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4941 -- Loss: 26.0226 ----- [+ -]             1.8043\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4942 - Loss Mean: 21.7294 ----- [+ -]         1.8847\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4942 -- Loss: 26.2528 ----- [+ -]             2.0294\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4943 - Loss Mean: 21.9559 ----- [+ -]         2.0771\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4943 -- Loss: 26.4864 ----- [+ -]             2.1927\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4944 - Loss Mean: 22.1963 ----- [+ -]         1.7810\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4944 -- Loss: 26.5959 ----- [+ -]             1.9118\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4945 - Loss Mean: 22.2891 ----- [+ -]         2.2473\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4945 -- Loss: 26.5448 ----- [+ -]             2.4447\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4946 - Loss Mean: 22.2076 ----- [+ -]         1.8958\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4946 -- Loss: 26.5509 ----- [+ -]             1.9638\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4947 - Loss Mean: 22.0513 ----- [+ -]         2.0371\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4947 -- Loss: 26.2786 ----- [+ -]             1.4937\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4948 - Loss Mean: 21.7547 ----- [+ -]         1.6770\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4948 -- Loss: 26.1640 ----- [+ -]             2.6181\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4949 - Loss Mean: 21.6922 ----- [+ -]         2.1226\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4949 -- Loss: 25.9408 ----- [+ -]             2.4765\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4950 - Loss Mean: 21.7132 ----- [+ -]         1.4978\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4950 -- Loss: 25.9229 ----- [+ -]             1.9079\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4951 - Loss Mean: 21.9774 ----- [+ -]         1.3668\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4951 -- Loss: 25.9475 ----- [+ -]             1.8648\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4952 - Loss Mean: 22.1476 ----- [+ -]         1.5782\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4952 -- Loss: 26.1451 ----- [+ -]             1.5638\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4953 - Loss Mean: 22.2345 ----- [+ -]         1.7458\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4953 -- Loss: 26.1999 ----- [+ -]             2.2617\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4954 - Loss Mean: 22.1830 ----- [+ -]         1.5963\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4954 -- Loss: 25.9424 ----- [+ -]             2.5815\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4955 - Loss Mean: 21.9403 ----- [+ -]         1.7957\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4955 -- Loss: 25.9748 ----- [+ -]             2.2670\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4956 - Loss Mean: 21.6798 ----- [+ -]         1.9831\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4956 -- Loss: 25.9864 ----- [+ -]             1.4483\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4957 - Loss Mean: 21.6864 ----- [+ -]         1.6585\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4957 -- Loss: 25.8828 ----- [+ -]             2.3235\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4958 - Loss Mean: 21.7080 ----- [+ -]         2.1486\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4958 -- Loss: 26.2379 ----- [+ -]             2.3826\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4959 - Loss Mean: 21.9452 ----- [+ -]         1.7846\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4959 -- Loss: 26.6321 ----- [+ -]             2.3808\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4960 - Loss Mean: 22.1399 ----- [+ -]         1.8256\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4960 -- Loss: 26.7097 ----- [+ -]             1.6867\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4961 - Loss Mean: 22.2883 ----- [+ -]         2.0292\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4961 -- Loss: 26.6063 ----- [+ -]             1.7632\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4962 - Loss Mean: 22.2074 ----- [+ -]         1.8963\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4962 -- Loss: 26.6804 ----- [+ -]             2.2457\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4963 - Loss Mean: 22.0103 ----- [+ -]         2.1323\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4963 -- Loss: 26.2889 ----- [+ -]             1.9096\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4964 - Loss Mean: 21.7274 ----- [+ -]         1.9455\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4964 -- Loss: 26.1122 ----- [+ -]             2.9215\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4965 - Loss Mean: 21.6459 ----- [+ -]         1.4840\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4965 -- Loss: 25.9452 ----- [+ -]             2.2456\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4966 - Loss Mean: 21.6579 ----- [+ -]         1.7184\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4966 -- Loss: 25.9480 ----- [+ -]             2.6424\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4967 - Loss Mean: 21.9532 ----- [+ -]         1.8715\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4967 -- Loss: 25.9340 ----- [+ -]             1.8195\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4968 - Loss Mean: 22.1681 ----- [+ -]         1.8015\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4968 -- Loss: 26.1524 ----- [+ -]             1.3430\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4969 - Loss Mean: 22.2703 ----- [+ -]         1.9079\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4969 -- Loss: 25.9232 ----- [+ -]             2.6642\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4970 - Loss Mean: 22.2312 ----- [+ -]         1.8094\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4970 -- Loss: 26.0608 ----- [+ -]             1.5039\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4971 - Loss Mean: 22.0206 ----- [+ -]         1.5610\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4971 -- Loss: 25.9495 ----- [+ -]             2.5227\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4972 - Loss Mean: 21.7289 ----- [+ -]         1.9320\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4972 -- Loss: 25.8677 ----- [+ -]             2.1454\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4973 - Loss Mean: 21.6490 ----- [+ -]         2.0110\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4973 -- Loss: 26.0582 ----- [+ -]             1.8081\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4974 - Loss Mean: 21.7112 ----- [+ -]         1.7915\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4974 -- Loss: 26.3460 ----- [+ -]             2.1403\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4975 - Loss Mean: 21.9209 ----- [+ -]         1.8232\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4975 -- Loss: 26.4948 ----- [+ -]             2.4250\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4976 - Loss Mean: 22.0624 ----- [+ -]         2.2642\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4976 -- Loss: 26.5846 ----- [+ -]             2.2259\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4977 - Loss Mean: 22.2099 ----- [+ -]         2.0285\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4977 -- Loss: 26.6073 ----- [+ -]             2.3855\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4978 - Loss Mean: 22.2174 ----- [+ -]         1.8500\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4978 -- Loss: 26.4588 ----- [+ -]             2.7417\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4979 - Loss Mean: 22.0389 ----- [+ -]         2.3008\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4979 -- Loss: 26.3630 ----- [+ -]             1.9013\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4980 - Loss Mean: 21.7072 ----- [+ -]         2.0891\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4980 -- Loss: 26.0892 ----- [+ -]             1.6358\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4981 - Loss Mean: 21.5595 ----- [+ -]         2.1720\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4981 -- Loss: 25.8389 ----- [+ -]             1.6683\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4982 - Loss Mean: 21.7479 ----- [+ -]         1.9908\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4982 -- Loss: 25.7607 ----- [+ -]             2.2159\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4983 - Loss Mean: 21.8596 ----- [+ -]         1.8112\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4983 -- Loss: 25.8933 ----- [+ -]             1.9837\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4984 - Loss Mean: 22.1510 ----- [+ -]         1.9003\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4984 -- Loss: 26.0647 ----- [+ -]             2.4047\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4985 - Loss Mean: 22.2555 ----- [+ -]         1.9148\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4985 -- Loss: 26.1427 ----- [+ -]             1.5854\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4986 - Loss Mean: 22.2296 ----- [+ -]         1.8571\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4986 -- Loss: 26.0708 ----- [+ -]             1.7675\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4987 - Loss Mean: 22.2206 ----- [+ -]         1.9406\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4987 -- Loss: 26.0629 ----- [+ -]             1.8200\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4988 - Loss Mean: 21.8776 ----- [+ -]         1.8463\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4988 -- Loss: 25.8827 ----- [+ -]             2.0139\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4989 - Loss Mean: 21.5908 ----- [+ -]         1.9187\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4989 -- Loss: 25.9402 ----- [+ -]             1.4308\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4990 - Loss Mean: 21.7360 ----- [+ -]         2.2911\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4990 -- Loss: 26.0272 ----- [+ -]             2.3330\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4991 - Loss Mean: 21.7630 ----- [+ -]         1.8362\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4991 -- Loss: 26.2196 ----- [+ -]             1.8127\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4992 - Loss Mean: 21.9554 ----- [+ -]         1.7280\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4992 -- Loss: 26.5819 ----- [+ -]             2.2588\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4993 - Loss Mean: 22.1987 ----- [+ -]         1.9323\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4993 -- Loss: 26.8376 ----- [+ -]             2.2048\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4994 - Loss Mean: 22.2361 ----- [+ -]         2.0376\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4994 -- Loss: 26.7642 ----- [+ -]             1.9695\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4995 - Loss Mean: 22.2021 ----- [+ -]         1.9896\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4995 -- Loss: 26.4944 ----- [+ -]             1.8088\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4996 - Loss Mean: 21.9661 ----- [+ -]         2.0131\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4996 -- Loss: 26.3447 ----- [+ -]             2.2819\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4997 - Loss Mean: 21.7453 ----- [+ -]         1.6942\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4997 -- Loss: 26.0153 ----- [+ -]             1.1739\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4998 - Loss Mean: 21.5595 ----- [+ -]         1.8965\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4998 -- Loss: 26.0131 ----- [+ -]             1.6973\n",
            "\n",
            "\n",
            "#################### -   Training   - ####################\n",
            "Epoch: 4999 - Loss Mean: 21.6593 ----- [+ -]         2.0407\n",
            "#################### -  Validation  - ####################\n",
            "Epoch: 4999 -- Loss: 25.8407 ----- [+ -]             2.0529\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing training and testing"
      ],
      "metadata": {
        "id": "8YHpmCpeucvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = torch.stack([tup[0] for tup in validation_set])\n",
        "Xtest = Xtest.to(args['device'])\n",
        "\n",
        "ytest = torch.stack([tup[1] for tup in validation_set])\n",
        "ypred = net(Xtest).cpu().data\n",
        "\n",
        "data_score = torch.cat((ytest, ypred), axis = 1)\n",
        "df_results = pd.DataFrame(data_score, columns = ['ypred', 'ytest'])\n",
        "df_results.head(50)"
      ],
      "metadata": {
        "id": "B4wnLJ1qY1sI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c80a02aa-76aa-4686-c31c-522d5723c7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    ypred       ytest\n",
              "0   352.0  343.011963\n",
              "1   156.0  126.373039\n",
              "2    12.0    7.531964\n",
              "3     2.0    4.915228\n",
              "4   391.0  331.765076\n",
              "5   391.0  333.306580\n",
              "6    84.0  103.485229\n",
              "7   487.0  456.321167\n",
              "8   176.0  220.318771\n",
              "9   157.0  185.922379\n",
              "10   82.0   75.369774\n",
              "11  186.0  168.431488\n",
              "12  277.0  290.896790\n",
              "13  264.0  295.187531\n",
              "14  312.0  293.916656\n",
              "15   56.0   74.229614\n",
              "16  370.0  363.099579\n",
              "17   14.0    5.662586\n",
              "18  124.0  122.425308\n",
              "19  427.0  438.337952\n",
              "20  132.0  142.540237\n",
              "21  377.0  420.548584\n",
              "22    7.0    9.228578\n",
              "23  665.0  559.066589\n",
              "24  103.0  138.886230\n",
              "25   70.0   58.078583\n",
              "26   41.0   45.239231\n",
              "27    1.0    4.352054\n",
              "28  122.0  218.944580\n",
              "29   92.0   58.797722\n",
              "30   22.0   14.788176\n",
              "31    8.0   16.418194\n",
              "32   90.0  112.822334\n",
              "33  151.0   83.973969\n",
              "34  285.0  318.404755\n",
              "35  272.0  331.408844\n",
              "36    2.0    9.271820\n",
              "37  180.0  154.961624\n",
              "38  482.0  457.094238\n",
              "39    1.0    4.320935\n",
              "40   11.0   12.158001\n",
              "41    4.0    8.241735\n",
              "42    4.0    5.403679\n",
              "43  113.0  136.207230\n",
              "44    3.0    4.147143\n",
              "45  300.0  267.774872\n",
              "46  228.0  228.976974\n",
              "47  330.0  287.779144\n",
              "48  539.0  525.194153\n",
              "49  104.0  107.185707"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31e6cf8f-1203-41c9-8979-ccb9eb3a87ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ypred</th>\n",
              "      <th>ytest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>352.0</td>\n",
              "      <td>343.011963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156.0</td>\n",
              "      <td>126.373039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.0</td>\n",
              "      <td>7.531964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4.915228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>391.0</td>\n",
              "      <td>331.765076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>391.0</td>\n",
              "      <td>333.306580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>84.0</td>\n",
              "      <td>103.485229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>487.0</td>\n",
              "      <td>456.321167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>176.0</td>\n",
              "      <td>220.318771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>157.0</td>\n",
              "      <td>185.922379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>82.0</td>\n",
              "      <td>75.369774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>186.0</td>\n",
              "      <td>168.431488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>277.0</td>\n",
              "      <td>290.896790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>264.0</td>\n",
              "      <td>295.187531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>312.0</td>\n",
              "      <td>293.916656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>56.0</td>\n",
              "      <td>74.229614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>370.0</td>\n",
              "      <td>363.099579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>14.0</td>\n",
              "      <td>5.662586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>124.0</td>\n",
              "      <td>122.425308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>427.0</td>\n",
              "      <td>438.337952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>132.0</td>\n",
              "      <td>142.540237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>377.0</td>\n",
              "      <td>420.548584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>7.0</td>\n",
              "      <td>9.228578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>665.0</td>\n",
              "      <td>559.066589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>103.0</td>\n",
              "      <td>138.886230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>70.0</td>\n",
              "      <td>58.078583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>41.0</td>\n",
              "      <td>45.239231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.352054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>122.0</td>\n",
              "      <td>218.944580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>92.0</td>\n",
              "      <td>58.797722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>22.0</td>\n",
              "      <td>14.788176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>8.0</td>\n",
              "      <td>16.418194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>90.0</td>\n",
              "      <td>112.822334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>151.0</td>\n",
              "      <td>83.973969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>285.0</td>\n",
              "      <td>318.404755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>272.0</td>\n",
              "      <td>331.408844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2.0</td>\n",
              "      <td>9.271820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>180.0</td>\n",
              "      <td>154.961624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>482.0</td>\n",
              "      <td>457.094238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.320935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>11.0</td>\n",
              "      <td>12.158001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4.0</td>\n",
              "      <td>8.241735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4.0</td>\n",
              "      <td>5.403679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>113.0</td>\n",
              "      <td>136.207230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>3.0</td>\n",
              "      <td>4.147143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>300.0</td>\n",
              "      <td>267.774872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>228.0</td>\n",
              "      <td>228.976974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>330.0</td>\n",
              "      <td>287.779144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>539.0</td>\n",
              "      <td>525.194153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>104.0</td>\n",
              "      <td>107.185707</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31e6cf8f-1203-41c9-8979-ccb9eb3a87ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-31e6cf8f-1203-41c9-8979-ccb9eb3a87ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-31e6cf8f-1203-41c9-8979-ccb9eb3a87ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-173d144b-4d68-4795-b0fb-e6d4d10d6567\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-173d144b-4d68-4795-b0fb-e6d4d10d6567')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-173d144b-4d68-4795-b0fb-e6d4d10d6567 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 3476,\n  \"fields\": [\n    {\n      \"column\": \"ypred\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 644,\n        \"samples\": [\n          505.0,\n          473.0,\n          977.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ytest\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3469,\n        \"samples\": [\n          120.24422454833984,\n          90.27403259277344,\n          214.4647216796875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 9))\n",
        "plt.plot(training_loss, label='Training')\n",
        "plt.plot(validation_loss, label='Validation', linewidth=3, alpha=0.5)\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.title('Convergence', fontsize=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9iIL7KS2Y1pf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "f51590c5-9953-4b0b-c1b8-3de08307676d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x900 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gn4X3mrvVhb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}